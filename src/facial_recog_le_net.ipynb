{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mily/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "caffe_root = '../../../caffe/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../data/train_data_cleaned_data_aug_flip_crop.npz\n",
      "loaded:  ['X_val_clean_cv', 'y_val_clean_cv', 'y_train_clean_cv', 'feature_labels', 'X_train_clean_cv']\n",
      "(13792, 1, 80, 80) (13792, 30)\n",
      "(3328, 1, 80, 80) (3328, 30)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "np_loaded_data_file = '../data/train_data_cleaned_data_aug_flip_crop.npz'\n",
    "if not os.path.isfile(np_loaded_data_file):\n",
    "    print \"%s does not exist. See facial_recog_kaggle.ipynb\" % np_loaded_data_file\n",
    "else:\n",
    "    print \"loading %s\" % np_loaded_data_file\n",
    "    npzfile = np.load(np_loaded_data_file)\n",
    "    print \"loaded: \", npzfile.files\n",
    "    X_train_clean_cv, y_train_clean_cv = npzfile['X_train_clean_cv'], npzfile['y_train_clean_cv']\n",
    "    X_val_clean_cv, y_val_clean_cv = npzfile['X_val_clean_cv'], npzfile['y_val_clean_cv']\n",
    "    feature_labels = npzfile['feature_labels']\n",
    "    print X_train_clean_cv.shape, y_train_clean_cv.shape\n",
    "    print X_val_clean_cv.shape, y_val_clean_cv.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__b_lr_5.00e-04__reg_param_0.00e+00__batch_size_64__drop_5.00e-01_201610282251\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "td_size = ( X_train_clean_cv.shape[0] // batch_size) * batch_size\n",
    "# print td_size\n",
    "# td_size = 256\n",
    "val_data_size, num_labels = y_val_clean_cv.shape\n",
    "\n",
    "\n",
    "# val_data_size = 2*batch_size\n",
    "\n",
    "\n",
    "#Define all params for training\n",
    "DEBUG_MSGS = 0\n",
    "dropout_ratio = 0.5\n",
    "num_epochs = 75\n",
    "min_epochs = 50\n",
    "auto_stop = False\n",
    "\n",
    "base_lr = 5e-4\n",
    "stepsize_in_epoch = 25 #drop learning rate once every stepsize_in_epoch epochs\n",
    "stepsize = (td_size // batch_size) * stepsize_in_epoch\n",
    "gamma = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reg_param = 0 * 4.641589e-03\n",
    "\n",
    "\n",
    "train_suffix = \"__b_lr_%.02e__reg_param_%0.02e__batch_size_%d__drop_%.02e_\" % (base_lr, reg_param, batch_size, dropout_ratio)\n",
    "train_suffix = train_suffix + time.strftime(\"%Y%m%d%H%M\")\n",
    "print train_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet(hdf5_list, batch_size=64, dropout_ratio=0.5, train=True):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5_list, ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=3, num_output=32, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu1 = L.ReLU(n.conv1, in_place=False, relu_param=dict(negative_slope=0.1))\n",
    "    n.pool1 = L.Pooling(n.relu1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.bn1   = L.BatchNorm(n.pool1, batch_norm_param=dict(use_global_stats = (train==False)))\n",
    "    \n",
    "    n.conv2 = L.Convolution(n.bn1, kernel_size=3, num_output=64, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu2 = L.ReLU(n.conv2, in_place=False, relu_param=dict(negative_slope=0.1))\n",
    "    n.pool2 = L.Pooling(n.relu2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.bn2   = L.BatchNorm(n.pool2, batch_norm_param=dict(use_global_stats = (train==False)))\n",
    "    \n",
    "    n.conv3 = L.Convolution(n.bn2, kernel_size=3, num_output=64, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu3 = L.ReLU(n.conv3, in_place=False, relu_param=dict(negative_slope=0.1))\n",
    "    n.pool3 = L.Pooling(n.relu3, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    \n",
    "    \n",
    "#     if train:\n",
    "    n.drop3 = fc1_input = L.Dropout(n.pool3, in_place=True, dropout_param = dict(dropout_ratio=dropout_ratio) )\n",
    "#     else:\n",
    "#         fc1_input = n.pool2\n",
    "            \n",
    "    n.fc1 =   L.InnerProduct(n.drop3, num_output=500, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu4 = L.ReLU(n.fc1, in_place=True, relu_param=dict(negative_slope=0.1))\n",
    "    n.score = L.InnerProduct(n.relu4, num_output=30, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.EuclideanLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "train_net_path = 'train_net' + train_suffix + '.prototxt'\n",
    "train_net_path_test_time = 'train_net_test_time' + train_suffix + '.prototxt'\n",
    "test_net_path = 'test_net' + train_suffix + '.prototxt'\n",
    "\n",
    "with open(train_net_path, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/train_hdf5.list', batch_size=batch_size, dropout_ratio=dropout_ratio, train=True)))\n",
    "    \n",
    "with open(train_net_path_test_time, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/train_hdf5.list', batch_size=batch_size, dropout_ratio=dropout_ratio, train=False)))\n",
    "    \n",
    "with open(test_net_path, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/test_hdf5.list', batch_size=batch_size, dropout_ratio=dropout_ratio, train=False)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "s = caffe_pb2.SolverParameter()\n",
    "\n",
    "# Set a seed for reproducible experiments:\n",
    "# this controls for randomization in training.\n",
    "s.random_seed = 0xCAFFE\n",
    "\n",
    "# Specify locations of the train and (maybe) test networks.\n",
    "s.train_net = train_net_path\n",
    "\n",
    "s.test_net.append(test_net_path)\n",
    "s.test_iter.append(1) # Test on 100 batches each time we test.\n",
    "\n",
    "s.test_net.append(train_net_path_test_time)\n",
    "s.test_iter.append(1) # Test on 100 batches each time we test.\n",
    "\n",
    "\n",
    "s.test_interval = 1000000  # Test after every s.test_interval training iterations.\n",
    "\n",
    "\n",
    "s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    " \n",
    "# EDIT HERE to try different solvers\n",
    "# solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "s.type = \"Adam\"\n",
    "\n",
    "# Set the initial learning rate for SGD.\n",
    "s.base_lr = base_lr  # EDIT HERE to try different learning rates\n",
    "# Set momentum to accelerate learning by\n",
    "# taking weighted average of current and previous updates.\n",
    "s.momentum = 0.9\n",
    "# Set weight decay to regularize and prevent overfitting\n",
    "s.weight_decay = reg_param\n",
    "\n",
    "# Set `lr_policy` to define how the learning rate changes during training.\n",
    "# This is the same policy as our default LeNet.\n",
    "# http://stackoverflow.com/questions/30033096/what-is-lr-policy-in-caffe\n",
    "# // The learning rate decay policy. The currently implemented learning rate\n",
    "# // policies are as follows:\n",
    "# //    - fixed: always return base_lr.\n",
    "# //    - step: return base_lr * gamma ^ (floor(iter / step))\n",
    "# //    - exp: return base_lr * gamma ^ iter\n",
    "# //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)\n",
    "# //    - multistep: similar to step but it allows non uniform steps defined by\n",
    "# //      stepvalue\n",
    "# //    - poly: the effective learning rate follows a polynomial decay, to be\n",
    "# //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)\n",
    "# //    - sigmoid: the effective learning rate follows a sigmod decay\n",
    "# //      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))\n",
    "# //\n",
    "# // where base_lr, max_iter, gamma, step, stepvalue and power are defined\n",
    "# // in the solver parameter protocol buffer, and iter is the current iteration.\n",
    "s.lr_policy = 'fixed'\n",
    "s.gamma = gamma\n",
    "s.power = 0.75\n",
    "s.stepsize = stepsize\n",
    "# EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "# `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "# s.lr_policy = 'fixed'\n",
    "\n",
    "# Display the current training loss and accuracy every 1000 iterations.\n",
    "s.display = 2\n",
    "\n",
    "# Snapshots are files used to store networks we've trained.\n",
    "# We'll snapshot every 5K iterations -- twice during training.\n",
    "s.snapshot = 1000000\n",
    "s.snapshot_prefix = 'lenet_'\n",
    "\n",
    "s.snapshot_after_train = True\n",
    "\n",
    "# Train on the GPU\n",
    "s.solver_mode = caffe_pb2.SolverParameter.CPU\n",
    "\n",
    "\n",
    "solver_config_fname = 'lenet_solver' + train_suffix + '.prototxt'\n",
    "# Write the solver to a temporary file and return its filename.\n",
    "with open(solver_config_fname, 'w') as f:\n",
    "    f.write(str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.8 s, sys: 778 ms, total: 32.6 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import h5py\n",
    "f = h5py.File('../data/train_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "f = h5py.File('../data/test_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "solver = None\n",
    "solver = caffe.get_solver(solver_config_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 1.06 s, total: 8.07 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 s, sys: 9.01 ms, total: 1.94 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 391 ms, total: 25.4 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_loss(a, b):\n",
    "    batch_size, num_labels = a.shape\n",
    "    \n",
    "    return np.sum((a-b)**2) / float(batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(a, b):\n",
    "    batch_size, num_labels = a.shape\n",
    "    return np.sqrt( np.sum((a-b)**2) / float(batch_size * num_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_size = 13760\n",
      "niter = 16125\n",
      "val_interval = 215\n",
      "it = 0, t_loss = 211.75, t_error = 225.24, v_error = 209.81, t_rmse = 3.88, v_rmse = 3.74\n",
      "it = 1, t_loss = 147.13, t_error = 119.60, v_error = 113.36, t_rmse = 2.82, v_rmse = 2.75\n",
      "it = 2, t_loss = 129.90, t_error = 88.92, v_error = 94.56, t_rmse = 2.43, v_rmse = 2.51\n",
      "it = 3, t_loss = 100.98, t_error = 70.45, v_error = 81.05, t_rmse = 2.17, v_rmse = 2.32\n",
      "it = 4, t_loss = 115.41, t_error = 80.63, v_error = 77.01, t_rmse = 2.32, v_rmse = 2.27\n",
      "it = 5, t_loss = 108.37, t_error = 90.25, v_error = 73.74, t_rmse = 2.45, v_rmse = 2.22\n",
      "it = 6, t_loss = 101.28, t_error = 70.38, v_error = 68.47, t_rmse = 2.17, v_rmse = 2.14\n",
      "it = 7, t_loss = 84.40, t_error = 57.55, v_error = 88.01, t_rmse = 1.96, v_rmse = 2.42\n",
      "it = 8, t_loss = 87.57, t_error = 68.14, v_error = 55.07, t_rmse = 2.13, v_rmse = 1.92\n",
      "it = 9, t_loss = 76.46, t_error = 51.55, v_error = 56.43, t_rmse = 1.85, v_rmse = 1.94\n",
      "it = 10, t_loss = 91.50, t_error = 66.95, v_error = 64.12, t_rmse = 2.11, v_rmse = 2.07\n",
      "it = 11, t_loss = 75.34, t_error = 45.53, v_error = 60.73, t_rmse = 1.74, v_rmse = 2.01\n",
      "it = 12, t_loss = 78.15, t_error = 39.69, v_error = 48.32, t_rmse = 1.63, v_rmse = 1.79\n",
      "it = 13, t_loss = 76.75, t_error = 45.21, v_error = 77.63, t_rmse = 1.74, v_rmse = 2.27\n",
      "it = 14, t_loss = 69.52, t_error = 42.44, v_error = 44.14, t_rmse = 1.68, v_rmse = 1.72\n",
      "it = 15, t_loss = 72.25, t_error = 50.60, v_error = 64.13, t_rmse = 1.84, v_rmse = 2.07\n",
      "it = 16, t_loss = 63.13, t_error = 42.12, v_error = 55.47, t_rmse = 1.68, v_rmse = 1.92\n",
      "it = 17, t_loss = 77.95, t_error = 47.96, v_error = 50.57, t_rmse = 1.79, v_rmse = 1.84\n",
      "it = 18, t_loss = 72.52, t_error = 40.99, v_error = 43.27, t_rmse = 1.65, v_rmse = 1.70\n",
      "it = 19, t_loss = 71.23, t_error = 34.20, v_error = 52.17, t_rmse = 1.51, v_rmse = 1.87\n",
      "it = 20, t_loss = 63.34, t_error = 34.41, v_error = 40.07, t_rmse = 1.51, v_rmse = 1.63\n",
      "it = 21, t_loss = 57.64, t_error = 39.71, v_error = 48.68, t_rmse = 1.63, v_rmse = 1.80\n",
      "it = 22, t_loss = 63.73, t_error = 35.17, v_error = 43.76, t_rmse = 1.53, v_rmse = 1.71\n",
      "it = 23, t_loss = 55.47, t_error = 38.86, v_error = 43.82, t_rmse = 1.61, v_rmse = 1.71\n",
      "it = 24, t_loss = 59.73, t_error = 37.82, v_error = 38.23, t_rmse = 1.59, v_rmse = 1.60\n",
      "it = 25, t_loss = 59.65, t_error = 38.46, v_error = 45.35, t_rmse = 1.60, v_rmse = 1.74\n",
      "it = 26, t_loss = 60.33, t_error = 31.47, v_error = 39.52, t_rmse = 1.45, v_rmse = 1.62\n",
      "it = 27, t_loss = 58.12, t_error = 34.34, v_error = 37.98, t_rmse = 1.51, v_rmse = 1.59\n",
      "it = 28, t_loss = 58.73, t_error = 41.62, v_error = 39.16, t_rmse = 1.67, v_rmse = 1.62\n",
      "it = 29, t_loss = 54.25, t_error = 31.82, v_error = 36.04, t_rmse = 1.46, v_rmse = 1.55\n",
      "it = 30, t_loss = 59.59, t_error = 27.50, v_error = 34.33, t_rmse = 1.35, v_rmse = 1.51\n",
      "it = 31, t_loss = 56.33, t_error = 28.50, v_error = 38.36, t_rmse = 1.38, v_rmse = 1.60\n",
      "it = 32, t_loss = 69.29, t_error = 27.29, v_error = 42.70, t_rmse = 1.35, v_rmse = 1.69\n",
      "it = 33, t_loss = 58.83, t_error = 31.13, v_error = 49.83, t_rmse = 1.44, v_rmse = 1.82\n",
      "it = 34, t_loss = 64.64, t_error = 27.88, v_error = 34.03, t_rmse = 1.36, v_rmse = 1.51\n",
      "it = 35, t_loss = 52.93, t_error = 23.76, v_error = 37.66, t_rmse = 1.26, v_rmse = 1.58\n",
      "it = 36, t_loss = 49.06, t_error = 28.42, v_error = 38.94, t_rmse = 1.38, v_rmse = 1.61\n",
      "it = 37, t_loss = 44.14, t_error = 23.38, v_error = 33.74, t_rmse = 1.25, v_rmse = 1.50\n",
      "it = 38, t_loss = 46.30, t_error = 22.91, v_error = 32.17, t_rmse = 1.24, v_rmse = 1.46\n",
      "it = 39, t_loss = 47.17, t_error = 25.25, v_error = 54.54, t_rmse = 1.30, v_rmse = 1.91\n",
      "it = 40, t_loss = 54.68, t_error = 23.71, v_error = 30.76, t_rmse = 1.26, v_rmse = 1.43\n",
      "it = 41, t_loss = 47.17, t_error = 23.96, v_error = 44.87, t_rmse = 1.26, v_rmse = 1.73\n",
      "it = 42, t_loss = 48.37, t_error = 29.83, v_error = 48.59, t_rmse = 1.41, v_rmse = 1.80\n",
      "it = 43, t_loss = 47.42, t_error = 22.80, v_error = 30.87, t_rmse = 1.23, v_rmse = 1.43\n",
      "it = 44, t_loss = 46.70, t_error = 23.67, v_error = 28.87, t_rmse = 1.26, v_rmse = 1.39\n",
      "it = 45, t_loss = 38.60, t_error = 23.65, v_error = 39.12, t_rmse = 1.26, v_rmse = 1.61\n",
      "it = 46, t_loss = 53.37, t_error = 24.16, v_error = 26.46, t_rmse = 1.27, v_rmse = 1.33\n",
      "it = 47, t_loss = 41.26, t_error = 27.18, v_error = 36.04, t_rmse = 1.35, v_rmse = 1.55\n",
      "it = 48, t_loss = 50.52, t_error = 24.24, v_error = 32.48, t_rmse = 1.27, v_rmse = 1.47\n",
      "it = 49, t_loss = 40.97, t_error = 20.18, v_error = 29.46, t_rmse = 1.16, v_rmse = 1.40\n",
      "it = 50, t_loss = 41.53, t_error = 23.11, v_error = 28.37, t_rmse = 1.24, v_rmse = 1.38\n",
      "it = 51, t_loss = 41.71, t_error = 23.24, v_error = 34.14, t_rmse = 1.24, v_rmse = 1.51\n",
      "it = 52, t_loss = 43.21, t_error = 37.87, v_error = 45.25, t_rmse = 1.59, v_rmse = 1.74\n",
      "it = 53, t_loss = 40.05, t_error = 22.15, v_error = 32.31, t_rmse = 1.22, v_rmse = 1.47\n",
      "it = 54, t_loss = 51.34, t_error = 33.12, v_error = 36.91, t_rmse = 1.49, v_rmse = 1.57\n",
      "it = 55, t_loss = 48.84, t_error = 19.50, v_error = 30.58, t_rmse = 1.14, v_rmse = 1.43\n",
      "it = 56, t_loss = 42.91, t_error = 20.54, v_error = 30.52, t_rmse = 1.17, v_rmse = 1.43\n",
      "it = 57, t_loss = 36.49, t_error = 24.64, v_error = 36.72, t_rmse = 1.28, v_rmse = 1.56\n",
      "it = 58, t_loss = 38.75, t_error = 18.83, v_error = 34.70, t_rmse = 1.12, v_rmse = 1.52\n",
      "it = 59, t_loss = 49.32, t_error = 25.78, v_error = 43.96, t_rmse = 1.31, v_rmse = 1.71\n",
      "it = 60, t_loss = 36.91, t_error = 23.14, v_error = 28.97, t_rmse = 1.24, v_rmse = 1.39\n",
      "it = 61, t_loss = 43.63, t_error = 24.52, v_error = 34.28, t_rmse = 1.28, v_rmse = 1.51\n",
      "it = 62, t_loss = 46.10, t_error = 27.20, v_error = 42.93, t_rmse = 1.35, v_rmse = 1.69\n",
      "it = 63, t_loss = 39.30, t_error = 18.83, v_error = 31.18, t_rmse = 1.12, v_rmse = 1.44\n",
      "it = 64, t_loss = 37.46, t_error = 18.90, v_error = 28.27, t_rmse = 1.12, v_rmse = 1.37\n",
      "it = 65, t_loss = 32.98, t_error = 24.09, v_error = 55.01, t_rmse = 1.27, v_rmse = 1.91\n",
      "it = 66, t_loss = 45.23, t_error = 20.60, v_error = 30.05, t_rmse = 1.17, v_rmse = 1.42\n",
      "it = 67, t_loss = 45.23, t_error = 20.16, v_error = 44.53, t_rmse = 1.16, v_rmse = 1.72\n",
      "it = 68, t_loss = 38.91, t_error = 17.37, v_error = 32.47, t_rmse = 1.08, v_rmse = 1.47\n",
      "it = 69, t_loss = 39.22, t_error = 19.91, v_error = 25.39, t_rmse = 1.15, v_rmse = 1.30\n",
      "it = 70, t_loss = 73.81, t_error = 30.37, v_error = 37.41, t_rmse = 1.42, v_rmse = 1.58\n",
      "it = 71, t_loss = 54.09, t_error = 36.30, v_error = 49.74, t_rmse = 1.56, v_rmse = 1.82\n",
      "it = 72, t_loss = 41.56, t_error = 23.64, v_error = 29.28, t_rmse = 1.26, v_rmse = 1.40\n",
      "it = 73, t_loss = 43.84, t_error = 23.34, v_error = 37.28, t_rmse = 1.25, v_rmse = 1.58\n",
      "it = 74, t_loss = 37.02, t_error = 14.34, v_error = 29.10, t_rmse = 0.98, v_rmse = 1.39\n",
      "CPU times: user 10h 46min 14s, sys: 9min 10s, total: 10h 55min 25s\n",
      "Wall time: 5h 30min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "f = h5py.File('../data/train_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "solver = None\n",
    "solver = caffe.get_solver(solver_config_fname)\n",
    "\n",
    "\n",
    "num_iter_per_epoch = int(np.ceil(float(td_size) / batch_size))\n",
    "niter = num_iter_per_epoch * num_epochs\n",
    "val_interval = num_iter_per_epoch\n",
    "\n",
    "\n",
    "# niter_val_error = int(np.ceil( float(val_data_size) / batch_size))\n",
    "niter_val_error = 2 \n",
    "niter_train_error = 2 \n",
    "\n",
    "print \"td_size = %d\" % td_size\n",
    "print \"niter = %d\" % niter\n",
    "print \"val_interval = %d\" % val_interval\n",
    "train_loss = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "train_error = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "val_error = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "train_rmse = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "val_rmse = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "\n",
    "\n",
    "conv1_out = []\n",
    "conv1_weights = []\n",
    "conv1_biases = []\n",
    "conv1_weights_diff = []\n",
    "conv1_biases_diff = []\n",
    "\n",
    "conv2_out = []\n",
    "conv2_weights = []\n",
    "conv2_biases = []\n",
    "conv2_weights_diff = []\n",
    "conv2_biases_diff = []\n",
    "\n",
    "\n",
    "fc1_weights = []\n",
    "fc1_biases = []\n",
    "fc1_weights_diff = []\n",
    "fc1_biases_diff = []\n",
    "\n",
    "score_weights = []\n",
    "score_biases = []\n",
    "score_weights_diff = []\n",
    "score_biases_diff = []\n",
    "\n",
    "out_score = []\n",
    "\n",
    "for it in range( int(np.ceil(float(niter) / val_interval)) ):\n",
    "    solver.step(num_iter_per_epoch)\n",
    "    \n",
    "    train_loss[it] = solver.net.blobs['loss'].data\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    if (it % val_interval) == 0 or (it == niter - 1) or True:\n",
    "        \n",
    "        if DEBUG_MSGS:\n",
    "        \n",
    "            conv1_out.append(solver.net.blobs['conv1'].data.copy())\n",
    "            conv2_out.append(solver.net.blobs['conv2'].data.copy())\n",
    "            out_score.append(solver.net.blobs['score'].data.copy())\n",
    "\n",
    "            score_weights.append(solver.net.params['score'][0].data.copy())\n",
    "            score_biases.append(solver.net.params['score'][1].data.copy())\n",
    "            score_weights_diff.append(solver.net.params['score'][0].diff.copy())\n",
    "            score_biases_diff.append(solver.net.params['score'][1].diff.copy())\n",
    "\n",
    "            conv1_weights.append(solver.net.params['conv1'][0].data.copy())\n",
    "            conv1_biases.append(solver.net.params['conv1'][1].data.copy())\n",
    "            conv1_weights_diff.append(solver.net.params['conv1'][0].diff.copy())\n",
    "            conv1_biases_diff.append(solver.net.params['conv1'][1].diff.copy())\n",
    "\n",
    "            conv2_weights.append(solver.net.params['conv2'][0].data.copy())\n",
    "            conv2_biases.append(solver.net.params['conv2'][1].data.copy())\n",
    "            conv2_weights_diff.append(solver.net.params['conv2'][0].diff.copy())\n",
    "            conv2_biases_diff.append(solver.net.params['conv2'][1].diff.copy())\n",
    "\n",
    "            fc1_weights.append(solver.net.params['fc1'][0].data.copy())\n",
    "            fc1_biases.append(solver.net.params['fc1'][1].data.copy())\n",
    "            fc1_weights_diff.append(solver.net.params['fc1'][0].diff.copy())\n",
    "            fc1_biases_diff.append(solver.net.params['fc1'][1].diff.copy())\n",
    "\n",
    "            score_weights.append(solver.net.params['score'][0].data.copy())\n",
    "            score_biases.append(solver.net.params['score'][1].data.copy())\n",
    "            score_weights_diff.append(solver.net.params['score'][0].diff.copy())\n",
    "            score_biases_diff.append(solver.net.params['score'][1].diff.copy())\n",
    "        \n",
    "        \n",
    "        val_error2_cum = 0\n",
    "        for test_it in range(niter_val_error):\n",
    "            solver.test_nets[0].forward()\n",
    "            val_error2_cum += np.sum ( (solver.test_nets[0].blobs['score'].data - \\\n",
    "                                        solver.test_nets[0].blobs['label'].data) ** 2)\n",
    "        val_error[it // val_interval] = val_error2_cum / (2 * batch_size * niter_val_error)\n",
    "        val_rmse[it // val_interval] = np.sqrt( val_error2_cum / (batch_size * niter_val_error * num_labels))\n",
    "        \n",
    "        train_error2_cum = 0\n",
    "        for test_it in range(niter_train_error):\n",
    "            solver.test_nets[1].forward()\n",
    "            train_error2_cum += np.sum( (solver.test_nets[1].blobs['score'].data - \\\n",
    "                                         solver.test_nets[1].blobs['label'].data) ** 2 )\n",
    "        train_error[it // val_interval] = train_error2_cum / (2 * batch_size * niter_val_error)\n",
    "        train_rmse[it // val_interval] = np.sqrt( train_error2_cum / (batch_size * niter_val_error * num_labels))\n",
    "        \n",
    "        \n",
    "        print \"it = %d, t_loss = %.02f, t_error = %.02f, v_error = %.02f, t_rmse = %.02f, v_rmse = %.02f\" % \\\n",
    "        (it, train_loss[it], \\\n",
    "         train_error[it // val_interval], val_error[it // val_interval], \\\n",
    "         train_rmse[it // val_interval], val_rmse[it // val_interval])\n",
    "        \n",
    "#         _, ax1 = plt.subplots()\n",
    "#         # ax2 = ax1.twinx()\n",
    "#         ax1.plot(range(niter), train_loss, 'k', label='train_loss')\n",
    "#         ax1.plot(val_interval * np.arange(len(val_error)), val_error, 'r', label='val_error')\n",
    "#         ax1.plot(val_interval * np.arange(len(train_error)), train_error, 'b', label='train_error')\n",
    "#         ax1.legend()\n",
    "#         ax1.set_xlabel('iteration')\n",
    "#         ax1.set_ylabel('error')\n",
    "#         ax1.set_ylim([0,1000])\n",
    "        \n",
    "        if (auto_stop == True) and (it >= min_epochs * batch_size):\n",
    "            val_it = it // val_interval\n",
    "            train_error_10 = train_error[val_it - 10: val_it]\n",
    "            mean_diff_10 = np.mean(np.diff(train_error_10))\n",
    "            if np.abs(mean_diff_10) < 5:\n",
    "                print \"mean_diff_10 = \", mean_diff_10\n",
    "                break\n",
    "\n",
    "solver.net.save('lenet_trained' + train_suffix + '.caffemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4bfdd0824500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ax2 = ax1.twinx()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_interval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_interval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mily/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mily/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mily/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mily/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mily/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must have same first dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFkCAYAAACuFXjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAFVhJREFUeJzt3X+M7XV95/HXmx+V4Ops7G3u1exNkKQiblN0RhpZVrcN\nFUqNRgIWB1jpxXXDQtPd6aY/kq6hkrTEtmDYDSy0sr2XqBNx/8K26SVQu9t4QevM4rZdQIPQBqtX\ntPayCij2fvaPc0aH2Tufe8+5M+fcuffxSE7CfOb7Pd/P+TjOPO/3e35Uay0AAOs5adoTAACObWIB\nAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUAoEssAABdYgEA6Bo5FqrqzVV1b1V9uaoOVtU7jmCf\nn6yqpap6vqq+UFVXjzddAGDSxjmz8NIkDye5LslhP1iiqs5I8odJHkhyTpJbk3y4qt46xrEBgAmr\no/kgqao6mOSdrbV7O9t8MMnFrbUfXzW2mGSmtfazYx8cAJiISTxn4U1J7l8ztjfJeRM4NgBwlE6Z\nwDF2JNm/Zmx/kpdX1Utaa99Zu0NV/XCSi5I8meT5TZ8hABw/TktyRpK9rbVvbMQdTiIWxnFRko9O\nexIAsIVdmeRjG3FHk4iFrybZvmZse5JnDnVWYejJJPnIRz6Ss88+exOnxmoLCwv50Ic+NO1pnFCs\n+eRZ88mz5pP1yCOP5KqrrkqGf0s3wiRi4cEkF68Zu3A4vp7nk+Tss8/O7OzsZs2LNWZmZqz3hFnz\nybPmk2fNp2bDLuOP8z4LL62qc6rq9cOhM4df7xx+/6aq2rNqlzuG23ywqs6qquuSXJbklqOePQCw\n6cZ5NcQbk/yvJEsZvM/CzUmWk3xg+P0dSXaubNxaezLJ25L8dAbvz7CQ5L2ttbWvkAAAjkEjX4Zo\nrf2PdCKjtbbrEGP/M8ncqMcCAKbPZ0PwffPz89OewgnHmk+eNZ88a771HdU7OG6WqppNsrS0tORJ\nMQAwguXl5czNzSXJXGtteSPu05kFAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUA\noEssAABdYgEA6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6xAIA\n0CUWAIAusQAAdIkFAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUAoEssAABdYgEA\n6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6xAIA0CUWAIAusQAA\ndIkFAKBrrFioquur6omqeq6qHqqqcw+z/ZVV9XBVfbuq/q6q7qqqV4w3ZQBgkkaOhaq6PMnNSW5I\n8oYkn0+yt6q2rbP9+Un2JPn9JK9LclmSn0jye2POGQCYoHHOLCwkubO1dndr7dEk1yZ5Nsk162z/\npiRPtNZua639TWttX5I7MwgGAOAYN1IsVNWpSeaSPLAy1lprSe5Pct46uz2YZGdVXTy8j+1J3pXk\nj8aZMAAwWaOeWdiW5OQk+9eM70+y41A7DM8kXJXk41X13SRfSfLNJL8w4rEBgCk4ZbMPUFWvS3Jr\nkt9Icl+SVyb53QwuRfyb3r4LCwuZmZl50dj8/Hzm5+c3Za4AsJUsLi5mcXHxRWMHDhzY8OPU4CrC\nEW48uAzxbJJLW2v3rhrfnWSmtXbJIfa5O8lprbWfWzV2fpI/T/LK1trasxSpqtkkS0tLS5mdnR3h\n4QDAiW15eTlzc3NJMtdaW96I+xzpMkRr7YUkS0kuWBmrqhp+vW+d3U5P8r01YweTtCQ1yvEBgMkb\n59UQtyR5X1W9p6pem+SODIJgd5JU1U1VtWfV9p9McmlVXVtVrx6eVbg1yWdaa189uukDAJtt5Ocs\ntNbuGb6nwo1Jtid5OMlFrbWnh5vsSLJz1fZ7quqfJLk+g+cq/EMGr6b4taOcOwAwAWM9wbG1dnuS\n29f53q5DjN2W5LZxjgUATJfPhgAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6\nxAIA0CUWAIAusQAAdIkFAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUAoEssAABd\nYgEA6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6xAIA0CUWAIAu\nsQAAdIkFAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUAoEssAABdYgEA6BILAECX\nWAAAusQCANAlFgCArrFioaqur6onquq5qnqoqs49zPY/VFW/WVVPVtXzVfWlqvr5sWYMAEzUKaPu\nUFWXJ7k5yb9N8tkkC0n2VtVrWmtfX2e3TyT5kSS7kjye5JVxVgMAtoSRYyGDOLiztXZ3klTVtUne\nluSaJL+9duOq+pkkb05yZmvtH4bDfzvedAGASRvpX/dVdWqSuSQPrIy11lqS+5Oct85ub0/yuSS/\nWlVPVdVjVfU7VXXamHMGACZo1DML25KcnGT/mvH9Sc5aZ58zMziz8HySdw7v478meUWS9454fABg\nwsa5DDGqk5IcTHJFa+1bSVJVv5TkE1V1XWvtO+vtuLCwkJmZmReNzc/PZ35+fjPnCwBbwuLiYhYX\nF180duDAgQ0/Tg2uIhzhxoPLEM8mubS1du+q8d1JZlprlxxin91J/kVr7TWrxl6b5K+TvKa19vgh\n9plNsrS0tJTZ2dkjfzQAcIJbXl7O3Nxcksy11pY34j5Hes5Ca+2FJEtJLlgZq6oafr1vnd0+neRV\nVXX6qrGzMjjb8NRIswUAJm6cly/ekuR9VfWe4RmCO5KcnmR3klTVTVW1Z9X2H0vyjSR/UFVnV9Vb\nMnjVxF29SxAAwLFh5OcstNbuqaptSW5Msj3Jw0kuaq09PdxkR5Kdq7b/dlW9Ncl/SfIXGYTDx5O8\n/yjnDgBMwFhPcGyt3Z7k9nW+t+sQY19IctE4xwIApsu7KAIAXWIBAOgSCwBAl1gAALrEAgDQJRYA\ngC6xAAB0iQUAoEssAABdYgEA6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsA\nQJdYAAC6xAIA0CUWAIAusQAAdIkFAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUA\noEssAABdYgEA6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6xAIA\n0CUWAIAusQAAdIkFAKBLLAAAXWIBAOgSCwBA11ixUFXXV9UTVfVcVT1UVece4X7nV9ULVbU8znEB\ngMkbORaq6vIkNye5Ickbknw+yd6q2naY/WaS7Ely/xjzBACmZJwzCwtJ7myt3d1aezTJtUmeTXLN\nYfa7I8lHkzw0xjEBgCkZKRaq6tQkc0keWBlrrbUMzhac19lvV5JXJ/nAeNMEAKbllBG335bk5CT7\n14zvT3LWoXaoqh9N8ltJ/mVr7WBVjTxJAGB6Ro2FkVTVSRlcerihtfb4yvCR7r+wsJCZmZkXjc3P\nz2d+fn7jJgkAW9Ti4mIWFxdfNHbgwIENP04NriIc4caDyxDPJrm0tXbvqvHdSWZaa5es2X4myTeT\nfC8/iISThv/9vSQXttb+7BDHmU2ytLS0lNnZ2VEeDwCc0JaXlzM3N5ckc621DXn14UjPWWitvZBk\nKckFK2M1uK5wQZJ9h9jlmSQ/luT1Sc4Z3u5I8ujwvz8z1qwBgIkZ5zLELUl2V9VSks9m8OqI05Ps\nTpKquinJq1prVw+f/Ph/Vu9cVV9L8nxr7ZGjmTgAMBkjx0Jr7Z7heyrcmGR7koeTXNRae3q4yY4k\nOzduigDANI31BMfW2u1Jbl/ne7sOs+8H4iWUALBl+GwIAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQ\nJRYAgC6xAAB0iQUAoEssAABdYgEA6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDo\nEgsAQJdYAAC6xAIA0CUWAIAusQAAdIkFAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0\niQUAoEssAABdYgEA6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6\nxAIA0CUWAIAusQAAdIkFAKBLLAAAXWIBAOgaKxaq6vqqeqKqnquqh6rq3M62l1TVfVX1tao6UFX7\nqurC8acMAEzSyLFQVZcnuTnJDUnekOTzSfZW1bZ1dnlLkvuSXJxkNsmnknyyqs4Za8YAwESNc2Zh\nIcmdrbW7W2uPJrk2ybNJrjnUxq21hdba77bWllprj7fWfj3JF5O8fexZAwATM1IsVNWpSeaSPLAy\n1lprSe5Pct4R3kcleVmSvx/l2ADAdIx6ZmFbkpOT7F8zvj/JjiO8j19O8tIk94x4bABgCk6Z5MGq\n6ook70/yjtba1w+3/cLCQmZmZl40Nj8/n/n5+U2aIQBsHYuLi1lcXHzR2IEDBzb8ODW4inCEGw8u\nQzyb5NLW2r2rxncnmWmtXdLZ991JPpzkstbanxzmOLNJlpaWljI7O3vE8wOAE93y8nLm5uaSZK61\ntrwR9znSZYjW2gtJlpJcsDI2fA7CBUn2rbdfVc0nuSvJuw8XCgDAsWWcyxC3JNldVUtJPpvBqyNO\nT7I7SarqpiSvaq1dPfz6iuH3fjHJX1TV9uH9PNdae+aoZg8AbLqRY6G1ds/wPRVuTLI9ycNJLmqt\nPT3cZEeSnat2eV8GT4q8bXhbsSfrvNwSADh2jPUEx9ba7UluX+d7u9Z8/VPjHAMAODb4bAgAoEss\nAABdYgEA6BILAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6xAIA0CUW\nAIAusQAAdIkFAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUAoEssAABdYgEA6BIL\nAECXWAAAusQCANAlFgCALrEAAHSJBQCgSywAAF1iAQDoEgsAQJdYAAC6xAIA0CUWAIAusQAAdIkF\nAKBLLAAAXWIBAOgSCwBAl1gAALrEAgDQJRYAgC6xAAB0iQUAoEssAABdYoHvW1xcnPYUTjjWfPKs\n+eRZ861vrFioquur6omqeq6qHqqqcw+z/U9W1VJVPV9VX6iqq8ebLpvJ/6Enz5pPnjWfPGu+9Y0c\nC1V1eZKbk9yQ5A1JPp9kb1VtW2f7M5L8YZIHkpyT5NYkH66qt443ZQBgksY5s7CQ5M7W2t2ttUeT\nXJvk2STXrLP9v0vypdbar7TWHmut3Zbkvw/vBwA4xo0UC1V1apK5DM4SJElaay3J/UnOW2e3Nw2/\nv9rezvYAwDHklBG335bk5CT714zvT3LWOvvsWGf7l1fVS1pr3znEPqclySOPPDLi9DgaBw4cyPLy\n8rSncUKx5pNnzSfPmk/Wqr+dp23UfY4aC5NyRpJcddVVU57GiWdubm7aUzjhWPPJs+aTZ82n4owk\n+zbijkaNha8n+cck29eMb0/y1XX2+eo62z+zzlmFZHCZ4sokTyZ5fsQ5AsCJ7LQMQmHvRt3hSLHQ\nWnuhqpaSXJDk3iSpqhp+/Z/X2e3BJBevGbtwOL7ecb6R5GOjzA0A+L4NOaOwYpxXQ9yS5H1V9Z6q\nem2SO5KcnmR3klTVTVW1Z9X2dyQ5s6o+WFVnVdV1SS4b3g8AcIwb+TkLrbV7hu+pcGMGlxMeTnJR\na+3p4SY7kuxctf2TVfW2JB9K8otJnkry3tba2ldIAADHoBq88hEA4NB8NgQA0CUWAICuqcSCD6Ka\nvFHWvKouqar7quprVXWgqvZV1YWTnO/xYNSf81X7nV9VL1SVd7EZ0Ri/W36oqn6zqp4c/n75UlX9\n/ISme1wYY82vrKqHq+rbVfV3VXVXVb1iUvPd6qrqzVV1b1V9uaoOVtU7jmCfo/4bOvFY8EFUkzfq\nmid5S5L7MnjJ62ySTyX5ZFWdM4HpHhfGWPOV/WaS7Mn//xbpHMaYa/6JJD+VZFeS1ySZT/LYJk/1\nuDHG7/PzM/j5/v0kr8vglXE/keT3JjLh48NLM3hhwXVJDvukww37G9pam+gtyUNJbl31dWXwColf\nWWf7Dyb532vGFpP88aTnvlVvo675OvfxV0n+07Qfy1a5jbvmw5/tD2Twy3d52o9jK93G+N3yM0n+\nPsk/nfbct+ptjDX/j0m+uGbsF5L87bQfy1a8JTmY5B2H2WZD/oZO9MyCD6KavDHXfO19VJKXZfCL\nlcMYd82raleSV2cQC4xgzDV/e5LPJfnVqnqqqh6rqt+pqg17P/3j2Zhr/mCSnVV18fA+tid5V5I/\n2tzZntA25G/opC9D9D6Iasc6+3Q/iGpjp3dcGmfN1/rlDE593bOB8zqejbzmVfWjSX4ryZWttYOb\nO73j0jg/52cmeXOSf57knUn+fQanxW/bpDkeb0Ze89baviRXJfl4VX03yVeSfDODswtsjg35G+rV\nEHRV1RVJ3p/kXa21r097PsejqjopyUeT3NBae3xleIpTOlGclMFp3Ctaa59rrf1Jkl9KcrV/iGyO\nqnpdBtfMfyOD50NdlMHZtDunOC2OwKQ/dXJSH0TFD4yz5kmSqnp3Bk88uqy19qnNmd5xadQ1f1mS\nNyZ5fVWt/Kv2pAyuAH03yYWttT/bpLkeL8b5Of9Kki+31r61auyRDELtnyV5/JB7sWKcNf+1JJ9u\nra283f9fDT8C4M+r6tdba2v/BczR25C/oRM9s9BaeyHJygdRJXnRB1Gt96EXD67efqj7QVT8wJhr\nnqqaT3JXkncP/8XFERpjzZ9J8mNJXp/Bs5XPyeAzVR4d/vdnNnnKW96YP+efTvKqqjp91dhZGZxt\neGqTpnrcGHPNT0/yvTVjBzN4Vr+zaZtjY/6GTuHZmz+X5Nkk70ny2gxOP30jyY8Mv39Tkj2rtj8j\nyf/N4BmdZ2XwcpHvJvnpaT8TdavcxljzK4ZrfG0GBbpye/m0H8tWuY265ofY36shNnnNM3gezt8k\n+XiSszN4yfBjSe6Y9mPZKrcx1vzqJN8Z/m55dZLzk3w2yb5pP5atchv+3J6TwT8uDib5D8Ovd66z\n5hvyN3RaD/a6JE8meS6Dunnjqu/9QZI/XbP9WzIo2OeSfDHJv572/2Bb7TbKmmfwvgr/eIjbf5v2\n49hKt1F/ztfsKxYmsOYZvLfC3iTfGobDbyd5ybQfx1a6jbHm1yf5y+GaP5XB+y68ctqPY6vckvyr\nYSQc8vfzZv0N9UFSAECXV0MAAF1iAQDoEgsAQJdYAAC6xAIA0CUWAIAusQAAdIkFAKBLLAAAXWIB\nAOgSCwBA1/8DBAe+bZJxNlwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1332def2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax1 = plt.subplots()\n",
    "# ax2 = ax1.twinx()\n",
    "ax1.plot(range(niter), train_loss, 'k', label='train_loss')\n",
    "ax1.plot(val_interval * np.arange(len(val_error)), val_error, 'r', label='val_error')\n",
    "ax1.plot(val_interval * np.arange(len(train_error)), train_error, 'b', label='train_error')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('iteration')\n",
    "ax1.set_ylabel('error')\n",
    "ax1.set_ylim([0,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine training auto stop \n",
    "\n",
    "mean_10 = np.zeros_like(train_error)\n",
    "std_10 = np.zeros_like(train_error)\n",
    "mean_diff_10 = np.zeros_like(train_error)\n",
    "\n",
    "for idx in range(10, len(train_error)):\n",
    "    train_error_10 = train_error[idx-10:idx]\n",
    "    mean_10[idx] = np.mean(train_error_10)\n",
    "    std_10[idx] = np.std(train_error_10)\n",
    "    mean_diff_10[idx] = np.abs(np.mean(np.diff(train_error_10)))\n",
    "\n",
    "plt.plot(range(len(train_error)), mean_10, color='b')\n",
    "plt.plot(range(len(train_error)), std_10, color='r')\n",
    "plt.plot(range(len(train_error)), mean_diff_10, color='k')\n",
    "plt.ylim([-10, 10])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"training_error = %f, val_error = %f\" % (train_error[-1], val_error[-1])\n",
    "print \"training_rmse = %f, val_rmse = %f\" % (train_rmse[-1], val_rmse[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    additional_epochs = 25\n",
    "    addn_niter = num_iter_per_epoch * additional_epochs\n",
    "    train_loss_2 = np.append(train_loss, np.zeros(addn_niter))\n",
    "    train_error_2 = np.append(train_error, np.zeros(int(np.ceil(float(addn_niter) / val_interval))))\n",
    "    val_error_2 = np.append(val_error, np.zeros(int(np.ceil(float(addn_niter) / val_interval))))\n",
    "\n",
    "    print \"niter = \", niter\n",
    "    print \"addn_niter = \", addn_niter\n",
    "\n",
    "    for it in range(niter, niter + addn_niter):\n",
    "        solver.step(1)\n",
    "\n",
    "        train_loss_2[it] = solver.net.blobs['loss'].data\n",
    "            \n",
    "        if (it % val_interval) == 0:\n",
    "\n",
    "            val_error_this = 0\n",
    "            for test_it in range(niter_val_error):\n",
    "                solver.test_nets[0].forward()\n",
    "                val_error_this += euclidean_loss(solver.test_nets[0].blobs['score'].data , \n",
    "                                                 solver.test_nets[0].blobs['label'].data) / niter_val_error\n",
    "            val_error_2[it // val_interval] = val_error_this\n",
    "\n",
    "            train_error_this = 0\n",
    "            for test_it in range(niter_train_error):\n",
    "                solver.test_nets[1].forward()\n",
    "                train_error_this += euclidean_loss(solver.test_nets[1].blobs['score'].data , \n",
    "                                                 solver.test_nets[1].blobs['label'].data) / niter_train_error\n",
    "            train_error_2[it // val_interval] = train_error_this\n",
    "\n",
    "\n",
    "            print \"addn_iter = %d, train_loss = %f, train_error = %f, val_error = %f\" % (it, train_loss_2[it], train_error_2[it // val_interval], val_error_2[it // val_interval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    _, ax1 = plt.subplots()\n",
    "    # ax2 = ax1.twinx()\n",
    "    ax1.plot(val_interval * np.arange(len(val_error_2)), train_error_2, label='train_error')\n",
    "    ax1.plot(val_interval * np.arange(len(val_error_2)), val_error_2, 'r', label='val_error')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('iteration')\n",
    "    ax1.set_ylabel('error')\n",
    "    ax1.set_ylim([0,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_net = solver.net\n",
    "train_error_sanity = 0\n",
    "niter_sanity = 1\n",
    "batch_size_used = 128\n",
    "for it in range(niter_sanity):\n",
    "    it_range = range(it*batch_size, it*batch_size+batch_size)\n",
    "    my_net.blobs['data'].data[...] = X_train_clean_cv[it_range]\n",
    "    my_net.forward()\n",
    "    out = my_net.blobs['score'].data\n",
    "    \n",
    "    net = my_net\n",
    "    \n",
    "    print \"data\"\n",
    "    print net.blobs['data'].data.shape\n",
    "    print net.blobs['data'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"conv1\"\n",
    "    print net.blobs['conv1'].data.shape\n",
    "    print net.blobs['conv1'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"relu1\"\n",
    "    print net.blobs['relu1'].data.shape\n",
    "    print net.blobs['relu1'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"pool1\"\n",
    "    print net.blobs['pool1'].data.shape\n",
    "    print net.blobs['pool1'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    \n",
    "    print \"conv2\"\n",
    "    print net.blobs['conv2'].data.shape\n",
    "    print net.blobs['conv2'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"relu2\"\n",
    "    print net.blobs['relu2'].data.shape\n",
    "    print net.blobs['relu2'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"pool2\"\n",
    "    print net.blobs['pool2'].data.shape\n",
    "    print net.blobs['pool2'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"fc1\"\n",
    "    print net.blobs['fc1'].data[0:2,0:5]\n",
    "    \n",
    "    print \"score\"\n",
    "    print net.blobs['score'].data[0:2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print \"conv1 weights\"\n",
    "    print net.params['conv1'][0].data.shape\n",
    "    print net.params['conv1'][0].data[0:2][0:5]\n",
    "    \n",
    "    print \"conv1 biases\"\n",
    "    print net.params['conv1'][1].data.shape\n",
    "    print net.params['conv1'][1].data[0:5]\n",
    "    \n",
    "    print \"conv2 weights\"\n",
    "    print net.params['conv2'][0].data.shape\n",
    "    print net.params['conv2'][0].data[0:2, 0:2, 0:5, 0]\n",
    "    \n",
    "    print \"conv2 biases\"\n",
    "    print net.params['conv2'][1].data.shape\n",
    "    print net.params['conv2'][1].data[0:5]\n",
    "    \n",
    "    print \"fc1 weights\"\n",
    "    print net.params['fc1'][0].data.shape\n",
    "    print net.params['fc1'][0].data[0:5, 0:5]\n",
    "    \n",
    "    print \"fc1 biases\"\n",
    "    print net.params['fc1'][1].data.shape\n",
    "    print net.params['fc1'][1].data[0:5]\n",
    "    \n",
    "    print \"score weights\"\n",
    "    print net.params['score'][0].data.shape\n",
    "    print net.params['score'][0].data[0:5, 0:5]\n",
    "    \n",
    "    print \"score biases\"\n",
    "    print net.params['score'][1].data.shape\n",
    "    print net.params['score'][1].data[0:5]\n",
    "    \n",
    "    \n",
    "    print \"diffs\"\n",
    "    print \"conv1 weights\"\n",
    "    print net.params['conv1'][0].diff.shape\n",
    "    print net.params['conv1'][0].diff[0:2][0:5]\n",
    "    \n",
    "    print \"conv1 biases\"\n",
    "    print net.params['conv1'][1].diff.shape\n",
    "    print net.params['conv1'][1].diff[0:5]\n",
    "    \n",
    "    print \"conv2 weights\"\n",
    "    print net.params['conv2'][0].diff.shape\n",
    "    print net.params['conv2'][0].diff[0:2, 0:2, 0:5, 0]\n",
    "    \n",
    "    print \"conv2 biases\"\n",
    "    print net.params['conv2'][1].diff.shape\n",
    "    print net.params['conv2'][1].diff[0:5]\n",
    "    \n",
    "    print \"fc1 weights\"\n",
    "    print net.params['fc1'][0].diff.shape\n",
    "    print net.params['fc1'][0].diff[0:5, 0:5]\n",
    "    \n",
    "    print \"fc1 biases\"\n",
    "    print net.params['fc1'][1].diff.shape\n",
    "    print net.params['fc1'][1].diff[0:5]\n",
    "    \n",
    "    print \"score weights\"\n",
    "    print net.params['score'][0].diff.shape\n",
    "    print net.params['score'][0].diff[0:5, 0:5]\n",
    "    \n",
    "    print \"score biases\"\n",
    "    print net.params['score'][1].diff.shape\n",
    "    print net.params['score'][1].diff[0:5]\n",
    "    \n",
    "#     print \"loss\"\n",
    "#     print net.params['loss'][0].diff.shape\n",
    "#     print net.params['loss'][0].diff[0:5]\n",
    "    \n",
    "#     train_error += np.sum( (out - y_train_clean_cv[it_range]) ** 2) / float(2* y_train_clean_cv.shape[1])\n",
    "    train_error_sanity += euclidean_loss(out, y_train_clean_cv[it_range])\n",
    "train_error_sanity = train_error_sanity / float(niter_sanity)  \n",
    "\n",
    "print train_error_sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_error_sanity = 0\n",
    "niter_sanity = 8\n",
    "batch_size_used = 64\n",
    "for it in range(niter_sanity):\n",
    "    it_range = range(it*batch_size, it*batch_size+batch_size)\n",
    "    my_net.blobs['data'].data[...] = X_val_clean_cv[it_range]\n",
    "    my_net.forward()\n",
    "    out = my_net.blobs['score'].data\n",
    "    \n",
    "#     train_error += np.sum( (out - y_train_clean_cv[it_range]) ** 2) / float(2* y_train_clean_cv.shape[1])\n",
    "    val_error_sanity += euclidean_loss(out, y_val_clean_cv[it_range])\n",
    "val_error_sanity = val_error_sanity / float(niter_sanity)\n",
    "\n",
    "print val_error_sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_error_sanity = 0\n",
    "niter_sanity = 8\n",
    "batch_size_used = 64\n",
    "for it in range(niter_sanity):\n",
    "    it_range = range(it*batch_size, it*batch_size+batch_size)\n",
    "    solver.test_nets[0].blobs['data'].data[...] = X_val_clean_cv[it_range]\n",
    "    solver.test_nets[0].forward(start='conv1')\n",
    "    out = solver.test_nets[0].blobs['score'].data\n",
    "    \n",
    "#     train_error += np.sum( (out - y_train_clean_cv[it_range]) ** 2) / float(2* y_train_clean_cv.shape[1])\n",
    "    val_error_sanity += np.sum ( (out - y_val_clean_cv[it_range]) ** 2)\n",
    "\n",
    "val_error_sanity = val_error_sanity / float(niter_sanity * 2 * batch_size_used)\n",
    "\n",
    "print val_error_sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG_MSGS:\n",
    "    print len(conv1_out)\n",
    "    print conv1_out[0].shape\n",
    "\n",
    "    for i in range(5):\n",
    "        conv1_out_i = conv1_out[i]\n",
    "        print conv1_out_i[0:5, 0, 0:5, 0]\n",
    "\n",
    "    print len(conv2_out)\n",
    "    print conv2_out[0].shape\n",
    "\n",
    "    for i in range(5):\n",
    "        conv2_out_i = conv2_out[i]\n",
    "        print conv2_out_i[0:5, 0, 0:5, 0]\n",
    "\n",
    "    print \"conv1 weights and biases\"\n",
    "    print len(conv1_weights)\n",
    "    print conv1_weights[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_weights_i = conv1_weights[i]\n",
    "        print conv1_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print len(conv1_biases)\n",
    "    print conv1_biases[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_biases_i = conv1_biases[i]\n",
    "        print conv1_biases_i[0:5]\n",
    "\n",
    "    print \"diff conv1 weights and biases\"\n",
    "    print len(conv1_weights_diff)\n",
    "    print conv1_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_weights_i = conv1_weights_diff[i]\n",
    "        print conv1_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print len(conv1_biases_diff)\n",
    "    print conv1_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_biases_i = conv1_biases_diff[i]\n",
    "        print conv1_biases_i[0:5]\n",
    "\n",
    "    print \"conv2 weights and biases\"\n",
    "    print len(conv2_weights)\n",
    "    print conv2_weights[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_weights_i = conv2_weights[i]\n",
    "        print conv2_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print len(conv2_biases)\n",
    "    print conv2_biases[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_biases_i = conv2_biases[i]\n",
    "        print conv2_biases_i[0:5]\n",
    "\n",
    "    print len(conv2_weights_diff)\n",
    "    print conv2_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_weights_i = conv2_weights_diff[i]\n",
    "        print conv2_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print \"diff conv2 weights and biases\"\n",
    "    print len(conv2_biases_diff)\n",
    "    print conv2_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_biases_i = conv2_biases_diff[i]\n",
    "        print conv2_biases_i[0:5]\n",
    "\n",
    "    print len(fc1_weights_diff)\n",
    "    print fc1_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        fc1_weights_i = fc1_weights_diff[i]\n",
    "        print fc1_weights_i[0:2,0:5]\n",
    "\n",
    "    print len(fc1_biases_diff)\n",
    "    print fc1_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        fc1_biases_i = fc1_biases_diff[i]\n",
    "        print fc1_biases_i[0:5]\n",
    "\n",
    "    print len(score_weights)\n",
    "    print score_weights[0].shape\n",
    "    for i in range(5):\n",
    "        score_weights_i = score_weights[i]\n",
    "        print score_weights_i[0:30:10,0]\n",
    "\n",
    "    print len(score_biases)\n",
    "    print score_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        score_biases_i = score_biases[i]\n",
    "        print score_biases_i[0:30:6]\n",
    "\n",
    "    print len(score_weights_diff)\n",
    "    print score_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        score_weights_i = score_weights_diff[i]\n",
    "        print score_weights_i[0:30:10,0]\n",
    "\n",
    "    print len(score_biases_diff)\n",
    "    print score_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        score_biases_i = score_biases_diff[i]\n",
    "        print score_biases_i[0:30:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

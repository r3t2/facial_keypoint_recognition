{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "caffe_root = '../../../caffe/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet(hdf5_list, batch_size=64):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5_list, ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.conv1, in_place=True)\n",
    "    n.pool1 = L.Pooling(n.relu1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    \n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
    "    n.relu2 = L.ReLU(n.conv2, in_place=True)\n",
    "    n.pool2 = L.Pooling(n.relu2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    \n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu3 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu3, num_output=30, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.EuclideanLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "train_net_path = 'train_net.prototxt'\n",
    "test_net_path = 'test_net.prototxt'\n",
    "batch_size = 64\n",
    "with open(train_net_path, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/train_hdf5.list', batch_size=batch_size)))\n",
    "    \n",
    "with open(test_net_path, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/test_hdf5.list', batch_size=batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "s = caffe_pb2.SolverParameter()\n",
    "\n",
    "# Set a seed for reproducible experiments:\n",
    "# this controls for randomization in training.\n",
    "s.random_seed = 0xCAFFE\n",
    "\n",
    "# Specify locations of the train and (maybe) test networks.\n",
    "s.train_net = train_net_path\n",
    "s.test_net.append(train_net_path)\n",
    "s.test_iter.append(1) # Test on 100 batches each time we test.\n",
    "\n",
    "s.test_net.append(test_net_path)\n",
    "s.test_iter.append(1) # Test on 100 batches each time we test.\n",
    "\n",
    "s.test_interval = 1000000  # Test after every 500 training iterations.\n",
    "\n",
    "\n",
    "s.max_iter = 100000     # no. of times to update the net (training iterations)\n",
    " \n",
    "# EDIT HERE to try different solvers\n",
    "# solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "s.type = \"Adam\"\n",
    "\n",
    "# Set the initial learning rate for SGD.\n",
    "s.base_lr = 0.0001  # EDIT HERE to try different learning rates\n",
    "# Set momentum to accelerate learning by\n",
    "# taking weighted average of current and previous updates.\n",
    "s.momentum = 0.9\n",
    "# Set weight decay to regularize and prevent overfitting\n",
    "s.weight_decay = 0\n",
    "\n",
    "# Set `lr_policy` to define how the learning rate changes during training.\n",
    "# This is the same policy as our default LeNet.\n",
    "s.lr_policy = 'fixed'\n",
    "s.gamma = 0.0001\n",
    "s.power = 0.75\n",
    "# EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "# `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "# s.lr_policy = 'fixed'\n",
    "\n",
    "# Display the current training loss and accuracy every 1000 iterations.\n",
    "s.display = 2\n",
    "\n",
    "# Snapshots are files used to store networks we've trained.\n",
    "# We'll snapshot every 5K iterations -- twice during training.\n",
    "s.snapshot = 1000000\n",
    "s.snapshot_prefix = 'lenet_'\n",
    "\n",
    "# Train on the GPU\n",
    "s.solver_mode = caffe_pb2.SolverParameter.CPU\n",
    "\n",
    "\n",
    "solver_config_fname = 'lenet_solver.prototxt'\n",
    "# Write the solver to a temporary file and return its filename.\n",
    "with open(solver_config_fname, 'w') as f:\n",
    "    f.write(str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../data/train_data_cleaned.npz\n",
      "loaded:  ['X_val_clean_cv', 'y_val_clean_cv', 'y_train_clean_cv', 'feature_labels', 'X_train_clean_cv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "np_loaded_data_file = '../data/train_data_cleaned.npz'\n",
    "if not os.path.isfile(np_loaded_data_file):\n",
    "    print \"%s does not exist. See facial_recog_kaggle.ipynb\" % np_loaded_data_file\n",
    "else:\n",
    "    print \"loading %s\" % np_loaded_data_file\n",
    "    npzfile = np.load(np_loaded_data_file)\n",
    "    print \"loaded: \", npzfile.files\n",
    "    X_train_clean_cv, y_train_clean_cv = npzfile['X_train_clean_cv'], npzfile['y_train_clean_cv']\n",
    "    X_val_clean_cv, y_val_clean_cv = npzfile['X_val_clean_cv'], npzfile['y_val_clean_cv']\n",
    "    feature_labels = npzfile['feature_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 108 ms, total: 3.95 s\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import h5py\n",
    "td_size = 64\n",
    "f = h5py.File('../data/train_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "f = h5py.File('../data/test_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "solver = None\n",
    "solver = caffe.get_solver(solver_config_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.85 s, sys: 284 ms, total: 6.13 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 s, sys: 44 ms, total: 3.62 s\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.2 s, sys: 487 ms, total: 45.7 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_size = 1\n",
      "niter = 100\n",
      "testing...\n",
      "train_error = 0.273949, val_error = 9371.415039\n",
      "CPU times: user 5min 51s, sys: 4.41 s, total: 5min 55s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "td_size = 1000\n",
    "f = h5py.File('../data/train_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "solver = None\n",
    "solver = caffe.get_solver(solver_config_fname)\n",
    "\n",
    "num_epochs = 10\n",
    "num_iter_per_epoch = int(np.ceil(float(td_size) / batch_size))\n",
    "niter = num_iter_per_epoch * num_epochs\n",
    "\n",
    "#val_data_size = X_val_clean_cv.shape[0]\n",
    "val_data_size = 2*batch_size\n",
    "\n",
    "print \"td_size = %d\" % td_size\n",
    "print \"niter = %d\" % niter\n",
    "\n",
    "solver.step(niter)\n",
    "\n",
    "print \"testing...\"\n",
    "train_error = 0\n",
    "for test_it in range(num_iter_per_epoch):\n",
    "    solver.test_nets[0].forward()\n",
    "    train_error += np.sum( (solver.test_nets[0].blobs['score'].data -\n",
    "                         solver.test_nets[0].blobs['label'].data) ** 2) / (float(num_iter_per_epoch * batch_size))\n",
    "\n",
    "val_error = 0\n",
    "for test_it in range(int(np.ceil( val_data_size / batch_size))):\n",
    "    solver.test_nets[1].forward()\n",
    "    val_error += np.sum( (solver.test_nets[1].blobs['score'].data -\n",
    "                         solver.test_nets[1].blobs['label'].data) ** 2) / (float(val_data_size))\n",
    "\n",
    "print \"train_error = %f, val_error = %f\" % (train_error, val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

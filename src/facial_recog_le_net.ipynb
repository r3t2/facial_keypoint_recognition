{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mily/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "caffe_root = '../../../caffe/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../data/train_data_cleaned_data_aug_flip_crop.npz\n",
      "loaded:  ['X_val_clean_cv', 'y_val_clean_cv', 'y_train_clean_cv', 'feature_labels', 'X_train_clean_cv']\n",
      "(13792, 1, 80, 80) (13792, 30)\n",
      "(3328, 1, 80, 80) (3328, 30)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "np_loaded_data_file = '../data/train_data_cleaned_data_aug_flip_crop.npz'\n",
    "# np_loaded_data_file = '../data/train_data_cleaned_4feat.npz'\n",
    "if not os.path.isfile(np_loaded_data_file):\n",
    "    print \"%s does not exist. See facial_recog_kaggle.ipynb\" % np_loaded_data_file\n",
    "else:\n",
    "    print \"loading %s\" % np_loaded_data_file\n",
    "    npzfile = np.load(np_loaded_data_file)\n",
    "    print \"loaded: \", npzfile.files\n",
    "    X_train_clean_cv, y_train_clean_cv = npzfile['X_train_clean_cv'], npzfile['y_train_clean_cv']\n",
    "    X_val_clean_cv, y_val_clean_cv = npzfile['X_val_clean_cv'], npzfile['y_val_clean_cv']\n",
    "    feature_labels = npzfile['feature_labels']\n",
    "    print X_train_clean_cv.shape, y_train_clean_cv.shape\n",
    "    print X_val_clean_cv.shape, y_val_clean_cv.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__b_lr_5.00e-04__batch_size_64__drop_5.00e-01_4feat_201611022331\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "td_size = ( X_train_clean_cv.shape[0] // batch_size) * batch_size\n",
    "# print td_size\n",
    "# td_size = 256\n",
    "val_data_size, num_labels = y_val_clean_cv.shape\n",
    "\n",
    "\n",
    "# val_data_size = 2*batch_size\n",
    "\n",
    "\n",
    "#Define all params for training\n",
    "DEBUG_MSGS = 0\n",
    "dropout_ratio = 0.5\n",
    "num_epochs = 150\n",
    "min_epochs = 50\n",
    "auto_stop = False\n",
    "\n",
    "base_lr = 5e-4\n",
    "stepsize_in_epoch = 25 #drop learning rate once every stepsize_in_epoch epochs\n",
    "stepsize = (td_size // batch_size) * stepsize_in_epoch\n",
    "gamma = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reg_param = 0 * 4.641589e-03\n",
    "\n",
    "\n",
    "train_suffix = \"__b_lr_%.02e__batch_size_%d__drop_%.02e_4feat_\" % (base_lr, batch_size, dropout_ratio)\n",
    "train_suffix = train_suffix + time.strftime(\"%Y%m%d%H%M\")\n",
    "print train_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet(hdf5_list, batch_size=64, dropout_ratio=0.5, train=True, num_output=30):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5_list, ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=3, num_output=32, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu1 = L.ReLU(n.conv1, in_place=False, relu_param=dict(negative_slope=0.1))\n",
    "    n.pool1 = L.Pooling(n.relu1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.bn1   = L.BatchNorm(n.pool1, batch_norm_param=dict(use_global_stats = (train==False)))\n",
    "    \n",
    "    n.conv2 = L.Convolution(n.bn1, kernel_size=3, num_output=64, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu2 = L.ReLU(n.conv2, in_place=False, relu_param=dict(negative_slope=0.1))\n",
    "    n.pool2 = L.Pooling(n.relu2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.bn2   = L.BatchNorm(n.pool2, batch_norm_param=dict(use_global_stats = (train==False)))\n",
    "    \n",
    "    n.conv3 = L.Convolution(n.bn2, kernel_size=3, num_output=64, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu3 = L.ReLU(n.conv3, in_place=False, relu_param=dict(negative_slope=0.1))\n",
    "    n.pool3 = L.Pooling(n.relu3, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    \n",
    "    \n",
    "#     if train:\n",
    "    n.drop3 = fc1_input = L.Dropout(n.pool3, in_place=True, dropout_param = dict(dropout_ratio=dropout_ratio) )\n",
    "#     else:\n",
    "#         fc1_input = n.pool2\n",
    "            \n",
    "    n.fc1 =   L.InnerProduct(n.drop3, num_output=500, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1))\n",
    "    n.relu4 = L.ReLU(n.fc1, in_place=True, relu_param=dict(negative_slope=0.1))\n",
    "    n.score = L.InnerProduct(n.relu4, num_output=num_output, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.EuclideanLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "\n",
    "train_net_path = 'train_net' + train_suffix + '.prototxt'\n",
    "train_net_path_test_time = 'train_net_test_time' + train_suffix + '.prototxt'\n",
    "test_net_path = 'test_net' + train_suffix + '.prototxt'\n",
    "\n",
    "with open(train_net_path, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/train_hdf5.list', batch_size=batch_size, dropout_ratio=dropout_ratio, train=True, num_output=num_labels)))\n",
    "    \n",
    "with open(train_net_path_test_time, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/train_hdf5.list', batch_size=batch_size, dropout_ratio=dropout_ratio, train=False, num_output=num_labels)))\n",
    "    \n",
    "with open(test_net_path, 'w') as f:\n",
    "    f.write(str(lenet(hdf5_list='../data/test_hdf5.list', batch_size=batch_size, dropout_ratio=dropout_ratio, train=False, num_output=num_labels)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "s = caffe_pb2.SolverParameter()\n",
    "\n",
    "# Set a seed for reproducible experiments:\n",
    "# this controls for randomization in training.\n",
    "s.random_seed = 0xCAFFE\n",
    "\n",
    "# Specify locations of the train and (maybe) test networks.\n",
    "s.train_net = train_net_path\n",
    "\n",
    "s.test_net.append(test_net_path)\n",
    "s.test_iter.append(1) # Test on 100 batches each time we test.\n",
    "\n",
    "s.test_net.append(train_net_path_test_time)\n",
    "s.test_iter.append(1) # Test on 100 batches each time we test.\n",
    "\n",
    "\n",
    "s.test_interval = 1000000  # Test after every s.test_interval training iterations.\n",
    "\n",
    "\n",
    "s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
    " \n",
    "# EDIT HERE to try different solvers\n",
    "# solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
    "s.type = \"Adam\"\n",
    "\n",
    "# Set the initial learning rate for SGD.\n",
    "s.base_lr = base_lr  # EDIT HERE to try different learning rates\n",
    "# Set momentum to accelerate learning by\n",
    "# taking weighted average of current and previous updates.\n",
    "s.momentum = 0.9\n",
    "# Set weight decay to regularize and prevent overfitting\n",
    "s.weight_decay = reg_param\n",
    "\n",
    "# Set `lr_policy` to define how the learning rate changes during training.\n",
    "# This is the same policy as our default LeNet.\n",
    "# http://stackoverflow.com/questions/30033096/what-is-lr-policy-in-caffe\n",
    "# // The learning rate decay policy. The currently implemented learning rate\n",
    "# // policies are as follows:\n",
    "# //    - fixed: always return base_lr.\n",
    "# //    - step: return base_lr * gamma ^ (floor(iter / step))\n",
    "# //    - exp: return base_lr * gamma ^ iter\n",
    "# //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)\n",
    "# //    - multistep: similar to step but it allows non uniform steps defined by\n",
    "# //      stepvalue\n",
    "# //    - poly: the effective learning rate follows a polynomial decay, to be\n",
    "# //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)\n",
    "# //    - sigmoid: the effective learning rate follows a sigmod decay\n",
    "# //      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))\n",
    "# //\n",
    "# // where base_lr, max_iter, gamma, step, stepvalue and power are defined\n",
    "# // in the solver parameter protocol buffer, and iter is the current iteration.\n",
    "s.lr_policy = 'fixed'\n",
    "s.gamma = gamma\n",
    "s.power = 0.75\n",
    "s.stepsize = stepsize\n",
    "# EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
    "# `fixed` is the simplest policy that keeps the learning rate constant.\n",
    "# s.lr_policy = 'fixed'\n",
    "\n",
    "# Display the current training loss and accuracy every 1000 iterations.\n",
    "s.display = 2\n",
    "\n",
    "# Snapshots are files used to store networks we've trained.\n",
    "# We'll snapshot every 5K iterations -- twice during training.\n",
    "s.snapshot = 1000000\n",
    "s.snapshot_prefix = 'lenet_'\n",
    "\n",
    "s.snapshot_after_train = True\n",
    "\n",
    "# Train on the GPU\n",
    "s.solver_mode = caffe_pb2.SolverParameter.CPU\n",
    "\n",
    "\n",
    "solver_config_fname = 'lenet_solver' + train_suffix + '.prototxt'\n",
    "# Write the solver to a temporary file and return its filename.\n",
    "with open(solver_config_fname, 'w') as f:\n",
    "    f.write(str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 1.24 s, total: 32.3 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import h5py\n",
    "f = h5py.File('../data/train_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "f = h5py.File('../data/test_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "solver = None\n",
    "solver = caffe.get_solver(solver_config_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.22 s, sys: 1.35 s, total: 9.57 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 s, sys: 40.1 ms, total: 1.9 s\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 375 ms, total: 24 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver.step(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_loss(a, b):\n",
    "    batch_size, num_labels = a.shape\n",
    "    \n",
    "    return np.sum((a-b)**2) / float(batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(a, b):\n",
    "    batch_size, num_labels = a.shape\n",
    "    return np.sqrt( np.sum((a-b)**2) / float(batch_size * num_labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td_size = 13760\n",
      "niter = 32250\n",
      "val_interval = 215\n",
      "it = 0, t_loss = 211.57, t_error = 224.08, v_error = 208.82, t_rmse = 3.87, v_rmse = 3.73\n",
      "it = 1, t_loss = 147.39, t_error = 120.44, v_error = 113.83, t_rmse = 2.83, v_rmse = 2.75\n",
      "it = 2, t_loss = 130.08, t_error = 89.43, v_error = 95.18, t_rmse = 2.44, v_rmse = 2.52\n",
      "it = 3, t_loss = 102.17, t_error = 71.27, v_error = 82.50, t_rmse = 2.18, v_rmse = 2.35\n",
      "it = 4, t_loss = 116.72, t_error = 80.59, v_error = 77.31, t_rmse = 2.32, v_rmse = 2.27\n",
      "it = 5, t_loss = 110.10, t_error = 91.08, v_error = 74.29, t_rmse = 2.46, v_rmse = 2.23\n",
      "it = 6, t_loss = 103.44, t_error = 70.78, v_error = 69.37, t_rmse = 2.17, v_rmse = 2.15\n",
      "it = 7, t_loss = 85.40, t_error = 58.44, v_error = 88.04, t_rmse = 1.97, v_rmse = 2.42\n",
      "it = 8, t_loss = 87.91, t_error = 69.31, v_error = 55.16, t_rmse = 2.15, v_rmse = 1.92\n",
      "it = 9, t_loss = 77.73, t_error = 52.30, v_error = 56.94, t_rmse = 1.87, v_rmse = 1.95\n",
      "it = 10, t_loss = 92.35, t_error = 67.76, v_error = 64.54, t_rmse = 2.13, v_rmse = 2.07\n",
      "it = 11, t_loss = 74.93, t_error = 45.80, v_error = 61.09, t_rmse = 1.75, v_rmse = 2.02\n",
      "it = 12, t_loss = 78.12, t_error = 39.95, v_error = 48.27, t_rmse = 1.63, v_rmse = 1.79\n",
      "it = 13, t_loss = 75.97, t_error = 45.95, v_error = 77.58, t_rmse = 1.75, v_rmse = 2.27\n",
      "it = 14, t_loss = 69.99, t_error = 42.56, v_error = 44.31, t_rmse = 1.68, v_rmse = 1.72\n",
      "it = 15, t_loss = 73.22, t_error = 52.45, v_error = 66.03, t_rmse = 1.87, v_rmse = 2.10\n",
      "it = 16, t_loss = 62.73, t_error = 43.61, v_error = 56.02, t_rmse = 1.71, v_rmse = 1.93\n",
      "it = 17, t_loss = 77.33, t_error = 48.39, v_error = 51.13, t_rmse = 1.80, v_rmse = 1.85\n",
      "it = 18, t_loss = 71.83, t_error = 39.89, v_error = 42.15, t_rmse = 1.63, v_rmse = 1.68\n",
      "it = 19, t_loss = 71.01, t_error = 34.34, v_error = 51.37, t_rmse = 1.51, v_rmse = 1.85\n",
      "it = 20, t_loss = 62.84, t_error = 34.09, v_error = 39.70, t_rmse = 1.51, v_rmse = 1.63\n",
      "it = 21, t_loss = 57.84, t_error = 39.26, v_error = 47.74, t_rmse = 1.62, v_rmse = 1.78\n",
      "it = 22, t_loss = 63.04, t_error = 35.06, v_error = 43.21, t_rmse = 1.53, v_rmse = 1.70\n",
      "it = 23, t_loss = 55.13, t_error = 42.52, v_error = 46.91, t_rmse = 1.68, v_rmse = 1.77\n",
      "it = 24, t_loss = 58.80, t_error = 38.03, v_error = 38.41, t_rmse = 1.59, v_rmse = 1.60\n",
      "it = 25, t_loss = 59.48, t_error = 35.80, v_error = 42.38, t_rmse = 1.54, v_rmse = 1.68\n",
      "it = 26, t_loss = 59.09, t_error = 29.91, v_error = 38.78, t_rmse = 1.41, v_rmse = 1.61\n",
      "it = 27, t_loss = 58.73, t_error = 35.61, v_error = 39.59, t_rmse = 1.54, v_rmse = 1.62\n",
      "it = 28, t_loss = 59.75, t_error = 43.17, v_error = 40.22, t_rmse = 1.70, v_rmse = 1.64\n",
      "it = 29, t_loss = 55.35, t_error = 30.88, v_error = 35.90, t_rmse = 1.43, v_rmse = 1.55\n",
      "it = 30, t_loss = 60.96, t_error = 27.38, v_error = 34.41, t_rmse = 1.35, v_rmse = 1.51\n",
      "it = 31, t_loss = 62.08, t_error = 29.05, v_error = 38.97, t_rmse = 1.39, v_rmse = 1.61\n",
      "it = 32, t_loss = 70.64, t_error = 27.78, v_error = 42.00, t_rmse = 1.36, v_rmse = 1.67\n",
      "it = 33, t_loss = 57.90, t_error = 30.49, v_error = 49.67, t_rmse = 1.43, v_rmse = 1.82\n",
      "it = 34, t_loss = 65.59, t_error = 27.30, v_error = 33.22, t_rmse = 1.35, v_rmse = 1.49\n",
      "it = 35, t_loss = 51.33, t_error = 23.00, v_error = 36.57, t_rmse = 1.24, v_rmse = 1.56\n",
      "it = 36, t_loss = 49.81, t_error = 26.80, v_error = 37.57, t_rmse = 1.34, v_rmse = 1.58\n",
      "it = 37, t_loss = 45.23, t_error = 22.55, v_error = 33.75, t_rmse = 1.23, v_rmse = 1.50\n",
      "it = 38, t_loss = 44.63, t_error = 22.68, v_error = 31.99, t_rmse = 1.23, v_rmse = 1.46\n",
      "it = 39, t_loss = 45.61, t_error = 25.81, v_error = 56.33, t_rmse = 1.31, v_rmse = 1.94\n",
      "it = 40, t_loss = 55.20, t_error = 25.20, v_error = 31.80, t_rmse = 1.30, v_rmse = 1.46\n",
      "it = 41, t_loss = 47.27, t_error = 23.15, v_error = 45.52, t_rmse = 1.24, v_rmse = 1.74\n",
      "it = 42, t_loss = 46.60, t_error = 34.31, v_error = 51.35, t_rmse = 1.51, v_rmse = 1.85\n",
      "it = 43, t_loss = 46.48, t_error = 20.60, v_error = 30.01, t_rmse = 1.17, v_rmse = 1.41\n",
      "it = 44, t_loss = 48.91, t_error = 21.86, v_error = 27.94, t_rmse = 1.21, v_rmse = 1.36\n",
      "it = 45, t_loss = 39.99, t_error = 23.00, v_error = 36.87, t_rmse = 1.24, v_rmse = 1.57\n",
      "it = 46, t_loss = 54.64, t_error = 24.88, v_error = 27.29, t_rmse = 1.29, v_rmse = 1.35\n",
      "it = 47, t_loss = 39.65, t_error = 27.20, v_error = 36.19, t_rmse = 1.35, v_rmse = 1.55\n",
      "it = 48, t_loss = 53.42, t_error = 24.36, v_error = 31.79, t_rmse = 1.27, v_rmse = 1.46\n",
      "it = 49, t_loss = 40.55, t_error = 19.00, v_error = 27.65, t_rmse = 1.13, v_rmse = 1.36\n",
      "it = 50, t_loss = 43.11, t_error = 25.38, v_error = 30.70, t_rmse = 1.30, v_rmse = 1.43\n",
      "it = 51, t_loss = 41.68, t_error = 24.69, v_error = 35.51, t_rmse = 1.28, v_rmse = 1.54\n",
      "it = 52, t_loss = 42.61, t_error = 38.46, v_error = 45.81, t_rmse = 1.60, v_rmse = 1.75\n",
      "it = 53, t_loss = 39.12, t_error = 22.61, v_error = 32.67, t_rmse = 1.23, v_rmse = 1.48\n",
      "it = 54, t_loss = 50.09, t_error = 27.07, v_error = 31.01, t_rmse = 1.34, v_rmse = 1.44\n",
      "it = 55, t_loss = 48.40, t_error = 19.87, v_error = 30.27, t_rmse = 1.15, v_rmse = 1.42\n",
      "it = 56, t_loss = 44.30, t_error = 22.67, v_error = 33.43, t_rmse = 1.23, v_rmse = 1.49\n",
      "it = 57, t_loss = 38.52, t_error = 21.85, v_error = 34.32, t_rmse = 1.21, v_rmse = 1.51\n",
      "it = 58, t_loss = 41.84, t_error = 19.54, v_error = 35.03, t_rmse = 1.14, v_rmse = 1.53\n",
      "it = 59, t_loss = 43.19, t_error = 26.18, v_error = 44.77, t_rmse = 1.32, v_rmse = 1.73\n",
      "it = 60, t_loss = 37.39, t_error = 23.48, v_error = 30.01, t_rmse = 1.25, v_rmse = 1.41\n",
      "it = 61, t_loss = 41.78, t_error = 26.40, v_error = 35.27, t_rmse = 1.33, v_rmse = 1.53\n",
      "it = 62, t_loss = 46.46, t_error = 24.61, v_error = 39.75, t_rmse = 1.28, v_rmse = 1.63\n",
      "it = 63, t_loss = 40.25, t_error = 19.02, v_error = 32.10, t_rmse = 1.13, v_rmse = 1.46\n",
      "it = 64, t_loss = 35.42, t_error = 17.35, v_error = 26.28, t_rmse = 1.08, v_rmse = 1.32\n",
      "it = 65, t_loss = 37.31, t_error = 19.15, v_error = 52.23, t_rmse = 1.13, v_rmse = 1.87\n",
      "it = 66, t_loss = 39.75, t_error = 24.96, v_error = 36.09, t_rmse = 1.29, v_rmse = 1.55\n",
      "it = 67, t_loss = 37.96, t_error = 18.94, v_error = 44.22, t_rmse = 1.12, v_rmse = 1.72\n",
      "it = 68, t_loss = 47.13, t_error = 17.96, v_error = 31.12, t_rmse = 1.09, v_rmse = 1.44\n",
      "it = 69, t_loss = 48.49, t_error = 19.50, v_error = 27.68, t_rmse = 1.14, v_rmse = 1.36\n",
      "it = 70, t_loss = 72.65, t_error = 31.83, v_error = 39.37, t_rmse = 1.46, v_rmse = 1.62\n",
      "it = 71, t_loss = 53.06, t_error = 31.06, v_error = 44.79, t_rmse = 1.44, v_rmse = 1.73\n",
      "it = 72, t_loss = 42.45, t_error = 19.56, v_error = 26.14, t_rmse = 1.14, v_rmse = 1.32\n",
      "it = 73, t_loss = 40.79, t_error = 20.93, v_error = 34.75, t_rmse = 1.18, v_rmse = 1.52\n",
      "it = 74, t_loss = 33.07, t_error = 14.74, v_error = 27.11, t_rmse = 0.99, v_rmse = 1.34\n",
      "it = 75, t_loss = 37.12, t_error = 15.66, v_error = 25.27, t_rmse = 1.02, v_rmse = 1.30\n",
      "it = 76, t_loss = 49.72, t_error = 23.19, v_error = 29.17, t_rmse = 1.24, v_rmse = 1.39\n",
      "it = 77, t_loss = 37.60, t_error = 14.99, v_error = 28.84, t_rmse = 1.00, v_rmse = 1.39\n",
      "it = 78, t_loss = 37.18, t_error = 20.79, v_error = 33.53, t_rmse = 1.18, v_rmse = 1.50\n",
      "it = 79, t_loss = 36.12, t_error = 37.38, v_error = 45.99, t_rmse = 1.58, v_rmse = 1.75\n",
      "it = 80, t_loss = 31.85, t_error = 16.23, v_error = 24.25, t_rmse = 1.04, v_rmse = 1.27\n",
      "it = 81, t_loss = 43.56, t_error = 19.53, v_error = 37.73, t_rmse = 1.14, v_rmse = 1.59\n",
      "it = 82, t_loss = 33.84, t_error = 17.62, v_error = 28.44, t_rmse = 1.08, v_rmse = 1.38\n",
      "it = 83, t_loss = 27.83, t_error = 15.84, v_error = 28.18, t_rmse = 1.03, v_rmse = 1.37\n",
      "it = 84, t_loss = 33.73, t_error = 14.77, v_error = 32.71, t_rmse = 0.99, v_rmse = 1.48\n",
      "it = 85, t_loss = 35.93, t_error = 13.23, v_error = 33.15, t_rmse = 0.94, v_rmse = 1.49\n",
      "it = 86, t_loss = 34.32, t_error = 15.41, v_error = 27.01, t_rmse = 1.01, v_rmse = 1.34\n",
      "it = 87, t_loss = 38.88, t_error = 19.29, v_error = 33.14, t_rmse = 1.13, v_rmse = 1.49\n",
      "it = 88, t_loss = 38.62, t_error = 20.41, v_error = 32.64, t_rmse = 1.17, v_rmse = 1.48\n",
      "it = 89, t_loss = 44.44, t_error = 15.29, v_error = 30.66, t_rmse = 1.01, v_rmse = 1.43\n",
      "it = 90, t_loss = 49.66, t_error = 18.40, v_error = 29.50, t_rmse = 1.11, v_rmse = 1.40\n",
      "it = 91, t_loss = 32.12, t_error = 17.38, v_error = 53.85, t_rmse = 1.08, v_rmse = 1.89\n",
      "it = 92, t_loss = 35.05, t_error = 14.12, v_error = 25.73, t_rmse = 0.97, v_rmse = 1.31\n",
      "it = 93, t_loss = 35.33, t_error = 23.08, v_error = 39.01, t_rmse = 1.24, v_rmse = 1.61\n",
      "it = 94, t_loss = 34.90, t_error = 15.90, v_error = 31.50, t_rmse = 1.03, v_rmse = 1.45\n",
      "it = 95, t_loss = 34.67, t_error = 16.69, v_error = 28.52, t_rmse = 1.05, v_rmse = 1.38\n",
      "it = 96, t_loss = 31.11, t_error = 16.54, v_error = 23.25, t_rmse = 1.05, v_rmse = 1.24\n",
      "it = 97, t_loss = 42.57, t_error = 36.42, v_error = 47.97, t_rmse = 1.56, v_rmse = 1.79\n",
      "it = 98, t_loss = 36.36, t_error = 13.75, v_error = 23.45, t_rmse = 0.96, v_rmse = 1.25\n",
      "it = 99, t_loss = 30.59, t_error = 13.53, v_error = 28.35, t_rmse = 0.95, v_rmse = 1.37\n",
      "it = 100, t_loss = 41.27, t_error = 13.02, v_error = 26.45, t_rmse = 0.93, v_rmse = 1.33\n",
      "it = 101, t_loss = 39.04, t_error = 19.09, v_error = 27.38, t_rmse = 1.13, v_rmse = 1.35\n",
      "it = 102, t_loss = 35.05, t_error = 13.73, v_error = 23.78, t_rmse = 0.96, v_rmse = 1.26\n",
      "it = 103, t_loss = 46.76, t_error = 14.60, v_error = 32.67, t_rmse = 0.99, v_rmse = 1.48\n",
      "it = 104, t_loss = 49.02, t_error = 47.19, v_error = 49.87, t_rmse = 1.77, v_rmse = 1.82\n",
      "it = 105, t_loss = 37.10, t_error = 45.58, v_error = 58.95, t_rmse = 1.74, v_rmse = 1.98\n",
      "it = 106, t_loss = 35.76, t_error = 31.32, v_error = 50.36, t_rmse = 1.44, v_rmse = 1.83\n",
      "it = 107, t_loss = 40.24, t_error = 21.94, v_error = 36.35, t_rmse = 1.21, v_rmse = 1.56\n",
      "it = 108, t_loss = 37.64, t_error = 16.83, v_error = 28.20, t_rmse = 1.06, v_rmse = 1.37\n",
      "it = 109, t_loss = 35.11, t_error = 12.79, v_error = 28.22, t_rmse = 0.92, v_rmse = 1.37\n",
      "it = 110, t_loss = 60.53, t_error = 20.52, v_error = 40.58, t_rmse = 1.17, v_rmse = 1.64\n",
      "it = 111, t_loss = 53.23, t_error = 16.04, v_error = 37.87, t_rmse = 1.03, v_rmse = 1.59\n",
      "it = 112, t_loss = 39.87, t_error = 16.96, v_error = 31.77, t_rmse = 1.06, v_rmse = 1.46\n",
      "it = 113, t_loss = 45.65, t_error = 17.44, v_error = 27.30, t_rmse = 1.08, v_rmse = 1.35\n",
      "it = 114, t_loss = 32.72, t_error = 14.20, v_error = 32.41, t_rmse = 0.97, v_rmse = 1.47\n",
      "it = 115, t_loss = 40.15, t_error = 20.25, v_error = 33.00, t_rmse = 1.16, v_rmse = 1.48\n",
      "it = 116, t_loss = 38.60, t_error = 17.76, v_error = 27.44, t_rmse = 1.09, v_rmse = 1.35\n",
      "it = 117, t_loss = 52.84, t_error = 19.86, v_error = 47.08, t_rmse = 1.15, v_rmse = 1.77\n",
      "it = 118, t_loss = 32.76, t_error = 13.54, v_error = 26.24, t_rmse = 0.95, v_rmse = 1.32\n",
      "it = 119, t_loss = 41.00, t_error = 18.29, v_error = 38.22, t_rmse = 1.10, v_rmse = 1.60\n",
      "it = 120, t_loss = 37.99, t_error = 16.83, v_error = 30.98, t_rmse = 1.06, v_rmse = 1.44\n",
      "it = 121, t_loss = 37.60, t_error = 14.27, v_error = 23.90, t_rmse = 0.98, v_rmse = 1.26\n",
      "it = 122, t_loss = 36.03, t_error = 14.89, v_error = 22.28, t_rmse = 1.00, v_rmse = 1.22\n",
      "it = 123, t_loss = 28.82, t_error = 14.28, v_error = 30.26, t_rmse = 0.98, v_rmse = 1.42\n",
      "it = 124, t_loss = 36.63, t_error = 14.66, v_error = 21.88, t_rmse = 0.99, v_rmse = 1.21\n",
      "it = 125, t_loss = 33.85, t_error = 13.70, v_error = 27.77, t_rmse = 0.96, v_rmse = 1.36\n",
      "it = 126, t_loss = 29.12, t_error = 21.12, v_error = 31.53, t_rmse = 1.19, v_rmse = 1.45\n",
      "it = 127, t_loss = 33.94, t_error = 14.79, v_error = 22.34, t_rmse = 0.99, v_rmse = 1.22\n",
      "it = 128, t_loss = 37.86, t_error = 21.62, v_error = 39.52, t_rmse = 1.20, v_rmse = 1.62\n",
      "it = 129, t_loss = 27.98, t_error = 28.09, v_error = 42.66, t_rmse = 1.37, v_rmse = 1.69\n",
      "it = 130, t_loss = 27.52, t_error = 19.62, v_error = 28.68, t_rmse = 1.14, v_rmse = 1.38\n",
      "it = 131, t_loss = 31.58, t_error = 19.24, v_error = 36.53, t_rmse = 1.13, v_rmse = 1.56\n",
      "it = 132, t_loss = 30.25, t_error = 15.93, v_error = 26.46, t_rmse = 1.03, v_rmse = 1.33\n",
      "it = 133, t_loss = 32.74, t_error = 21.82, v_error = 39.33, t_rmse = 1.21, v_rmse = 1.62\n",
      "it = 134, t_loss = 25.25, t_error = 22.41, v_error = 29.28, t_rmse = 1.22, v_rmse = 1.40\n",
      "it = 135, t_loss = 31.09, t_error = 15.77, v_error = 27.48, t_rmse = 1.03, v_rmse = 1.35\n",
      "it = 136, t_loss = 28.53, t_error = 14.50, v_error = 33.50, t_rmse = 0.98, v_rmse = 1.49\n",
      "it = 137, t_loss = 31.56, t_error = 21.74, v_error = 42.79, t_rmse = 1.20, v_rmse = 1.69\n",
      "it = 138, t_loss = 30.29, t_error = 26.23, v_error = 38.39, t_rmse = 1.32, v_rmse = 1.60\n",
      "it = 139, t_loss = 31.76, t_error = 17.12, v_error = 30.42, t_rmse = 1.07, v_rmse = 1.42\n",
      "it = 140, t_loss = 32.16, t_error = 29.67, v_error = 47.20, t_rmse = 1.41, v_rmse = 1.77\n",
      "it = 141, t_loss = 33.96, t_error = 22.21, v_error = 30.62, t_rmse = 1.22, v_rmse = 1.43\n",
      "it = 142, t_loss = 30.63, t_error = 24.63, v_error = 32.98, t_rmse = 1.28, v_rmse = 1.48\n",
      "it = 143, t_loss = 32.11, t_error = 26.59, v_error = 58.51, t_rmse = 1.33, v_rmse = 1.97\n",
      "it = 144, t_loss = 37.84, t_error = 20.86, v_error = 30.97, t_rmse = 1.18, v_rmse = 1.44\n",
      "it = 145, t_loss = 39.22, t_error = 11.97, v_error = 35.26, t_rmse = 0.89, v_rmse = 1.53\n",
      "it = 146, t_loss = 37.67, t_error = 12.34, v_error = 28.37, t_rmse = 0.91, v_rmse = 1.38\n",
      "it = 147, t_loss = 24.72, t_error = 11.01, v_error = 21.92, t_rmse = 0.86, v_rmse = 1.21\n",
      "it = 148, t_loss = 32.45, t_error = 13.29, v_error = 22.79, t_rmse = 0.94, v_rmse = 1.23\n",
      "it = 149, t_loss = 31.65, t_error = 19.38, v_error = 38.57, t_rmse = 1.14, v_rmse = 1.60\n",
      "CPU times: user 1d 3h 8min 18s, sys: 27min 59s, total: 1d 3h 36min 17s\n",
      "Wall time: 13h 55min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "f = h5py.File('../data/train_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_train_clean_cv[0:td_size], compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "solver = None\n",
    "solver = caffe.get_solver(solver_config_fname)\n",
    "\n",
    "\n",
    "num_iter_per_epoch = int(np.ceil(float(td_size) / batch_size))\n",
    "niter = num_iter_per_epoch * num_epochs\n",
    "val_interval = num_iter_per_epoch\n",
    "\n",
    "\n",
    "# niter_val_error = int(np.ceil( float(val_data_size) / batch_size))\n",
    "niter_val_error = 2 \n",
    "niter_train_error = 2 \n",
    "\n",
    "print \"td_size = %d\" % td_size\n",
    "print \"niter = %d\" % niter\n",
    "print \"val_interval = %d\" % val_interval\n",
    "train_loss = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "train_error = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "val_error = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "train_rmse = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "val_rmse = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "\n",
    "\n",
    "conv1_out = []\n",
    "conv1_weights = []\n",
    "conv1_biases = []\n",
    "conv1_weights_diff = []\n",
    "conv1_biases_diff = []\n",
    "\n",
    "conv2_out = []\n",
    "conv2_weights = []\n",
    "conv2_biases = []\n",
    "conv2_weights_diff = []\n",
    "conv2_biases_diff = []\n",
    "\n",
    "\n",
    "fc1_weights = []\n",
    "fc1_biases = []\n",
    "fc1_weights_diff = []\n",
    "fc1_biases_diff = []\n",
    "\n",
    "score_weights = []\n",
    "score_biases = []\n",
    "score_weights_diff = []\n",
    "score_biases_diff = []\n",
    "\n",
    "out_score = []\n",
    "\n",
    "for it in range( int(np.ceil(float(niter) / val_interval)) ):\n",
    "    solver.step(num_iter_per_epoch)\n",
    "    \n",
    "    train_loss[it] = solver.net.blobs['loss'].data\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    if (it % val_interval) == 0 or (it == niter - 1) or True:\n",
    "        \n",
    "        if DEBUG_MSGS:\n",
    "        \n",
    "            conv1_out.append(solver.net.blobs['conv1'].data.copy())\n",
    "            conv2_out.append(solver.net.blobs['conv2'].data.copy())\n",
    "            out_score.append(solver.net.blobs['score'].data.copy())\n",
    "\n",
    "            score_weights.append(solver.net.params['score'][0].data.copy())\n",
    "            score_biases.append(solver.net.params['score'][1].data.copy())\n",
    "            score_weights_diff.append(solver.net.params['score'][0].diff.copy())\n",
    "            score_biases_diff.append(solver.net.params['score'][1].diff.copy())\n",
    "\n",
    "            conv1_weights.append(solver.net.params['conv1'][0].data.copy())\n",
    "            conv1_biases.append(solver.net.params['conv1'][1].data.copy())\n",
    "            conv1_weights_diff.append(solver.net.params['conv1'][0].diff.copy())\n",
    "            conv1_biases_diff.append(solver.net.params['conv1'][1].diff.copy())\n",
    "\n",
    "            conv2_weights.append(solver.net.params['conv2'][0].data.copy())\n",
    "            conv2_biases.append(solver.net.params['conv2'][1].data.copy())\n",
    "            conv2_weights_diff.append(solver.net.params['conv2'][0].diff.copy())\n",
    "            conv2_biases_diff.append(solver.net.params['conv2'][1].diff.copy())\n",
    "\n",
    "            fc1_weights.append(solver.net.params['fc1'][0].data.copy())\n",
    "            fc1_biases.append(solver.net.params['fc1'][1].data.copy())\n",
    "            fc1_weights_diff.append(solver.net.params['fc1'][0].diff.copy())\n",
    "            fc1_biases_diff.append(solver.net.params['fc1'][1].diff.copy())\n",
    "\n",
    "            score_weights.append(solver.net.params['score'][0].data.copy())\n",
    "            score_biases.append(solver.net.params['score'][1].data.copy())\n",
    "            score_weights_diff.append(solver.net.params['score'][0].diff.copy())\n",
    "            score_biases_diff.append(solver.net.params['score'][1].diff.copy())\n",
    "        \n",
    "        \n",
    "        val_error2_cum = 0\n",
    "        for test_it in range(niter_val_error):\n",
    "            solver.test_nets[0].forward()\n",
    "            val_error2_cum += np.sum ( (solver.test_nets[0].blobs['score'].data - \\\n",
    "                                        solver.test_nets[0].blobs['label'].data) ** 2)\n",
    "        val_error[it // val_interval] = val_error2_cum / (2 * batch_size * niter_val_error)\n",
    "        val_rmse[it // val_interval] = np.sqrt( val_error2_cum / (batch_size * niter_val_error * num_labels))\n",
    "        \n",
    "        train_error2_cum = 0\n",
    "        for test_it in range(niter_train_error):\n",
    "            solver.test_nets[1].forward()\n",
    "            train_error2_cum += np.sum( (solver.test_nets[1].blobs['score'].data - \\\n",
    "                                         solver.test_nets[1].blobs['label'].data) ** 2 )\n",
    "        train_error[it // val_interval] = train_error2_cum / (2 * batch_size * niter_val_error)\n",
    "        train_rmse[it // val_interval] = np.sqrt( train_error2_cum / (batch_size * niter_val_error * num_labels))\n",
    "        \n",
    "        \n",
    "        print \"it = %d, t_loss = %.02f, t_error = %.02f, v_error = %.02f, t_rmse = %.02f, v_rmse = %.02f\" % \\\n",
    "        (it, train_loss[it], \\\n",
    "         train_error[it // val_interval], val_error[it // val_interval], \\\n",
    "         train_rmse[it // val_interval], val_rmse[it // val_interval])\n",
    "        \n",
    "#         _, ax1 = plt.subplots()\n",
    "#         # ax2 = ax1.twinx()\n",
    "#         ax1.plot(range(niter), train_loss, 'k', label='train_loss')\n",
    "#         ax1.plot(val_interval * np.arange(len(val_error)), val_error, 'r', label='val_error')\n",
    "#         ax1.plot(val_interval * np.arange(len(train_error)), train_error, 'b', label='train_error')\n",
    "#         ax1.legend()\n",
    "#         ax1.set_xlabel('iteration')\n",
    "#         ax1.set_ylabel('error')\n",
    "#         ax1.set_ylim([0,1000])\n",
    "        \n",
    "        if (auto_stop == True) and (it >= min_epochs * batch_size):\n",
    "            val_it = it // val_interval\n",
    "            train_error_10 = train_error[val_it - 10: val_it]\n",
    "            mean_diff_10 = np.mean(np.diff(train_error_10))\n",
    "            if np.abs(mean_diff_10) < 5:\n",
    "                print \"mean_diff_10 = \", mean_diff_10\n",
    "                break\n",
    "\n",
    "solver.net.save('lenet_trained' + train_suffix + '.caffemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8X2orv8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefd78cd590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax1 = plt.subplots()\n",
    "# ax2 = ax1.twinx()\n",
    "ax1.plot(val_interval * np.arange(len(train_loss)), train_loss, 'k', label='train_loss')\n",
    "ax1.plot(val_interval * np.arange(len(val_error)), val_error, 'r', label='val_error')\n",
    "ax1.plot(val_interval * np.arange(len(train_error)), train_error, 'b', label='train_error')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('iteration')\n",
    "ax1.set_ylabel('error')\n",
    "ax1.set_ylim([0,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhUAAAFkCAYAAACXcsmHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqAAAuvj/jjh6Ui2/1GUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefbd832e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine training auto stop \n",
    "\n",
    "mean_10 = np.zeros_like(train_error)\n",
    "std_10 = np.zeros_like(train_error)\n",
    "mean_diff_10 = np.zeros_like(train_error)\n",
    "\n",
    "for idx in range(10, len(train_error)):\n",
    "    train_error_10 = train_error[idx-10:idx]\n",
    "    mean_10[idx] = np.mean(train_error_10)\n",
    "    std_10[idx] = np.std(train_error_10)\n",
    "    mean_diff_10[idx] = np.abs(np.mean(np.diff(train_error_10)))\n",
    "\n",
    "plt.plot(range(len(train_error)), mean_10, color='b')\n",
    "plt.plot(range(len(train_error)), std_10, color='r')\n",
    "plt.plot(range(len(train_error)), mean_diff_10, color='k')\n",
    "plt.ylim([-10, 10])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_error = 0.000000, val_error = 0.000000\n",
      "training_rmse = 0.000000, val_rmse = 0.000000\n"
     ]
    }
   ],
   "source": [
    "print \"training_error = %f, val_error = %f\" % (train_error[-1], val_error[-1])\n",
    "print \"training_rmse = %f, val_rmse = %f\" % (train_rmse[-1], val_rmse[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    additional_epochs = 25\n",
    "    addn_niter = num_iter_per_epoch * additional_epochs\n",
    "    train_loss_2 = np.append(train_loss, np.zeros(addn_niter))\n",
    "    train_error_2 = np.append(train_error, np.zeros(int(np.ceil(float(addn_niter) / val_interval))))\n",
    "    val_error_2 = np.append(val_error, np.zeros(int(np.ceil(float(addn_niter) / val_interval))))\n",
    "\n",
    "    print \"niter = \", niter\n",
    "    print \"addn_niter = \", addn_niter\n",
    "\n",
    "    for it in range(niter, niter + addn_niter):\n",
    "        solver.step(1)\n",
    "\n",
    "        train_loss_2[it] = solver.net.blobs['loss'].data\n",
    "            \n",
    "        if (it % val_interval) == 0:\n",
    "\n",
    "            val_error_this = 0\n",
    "            for test_it in range(niter_val_error):\n",
    "                solver.test_nets[0].forward()\n",
    "                val_error_this += euclidean_loss(solver.test_nets[0].blobs['score'].data , \n",
    "                                                 solver.test_nets[0].blobs['label'].data) / niter_val_error\n",
    "            val_error_2[it // val_interval] = val_error_this\n",
    "\n",
    "            train_error_this = 0\n",
    "            for test_it in range(niter_train_error):\n",
    "                solver.test_nets[1].forward()\n",
    "                train_error_this += euclidean_loss(solver.test_nets[1].blobs['score'].data , \n",
    "                                                 solver.test_nets[1].blobs['label'].data) / niter_train_error\n",
    "            train_error_2[it // val_interval] = train_error_this\n",
    "\n",
    "\n",
    "            print \"addn_iter = %d, train_loss = %f, train_error = %f, val_error = %f\" % (it, train_loss_2[it], train_error_2[it // val_interval], val_error_2[it // val_interval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    _, ax1 = plt.subplots()\n",
    "    # ax2 = ax1.twinx()\n",
    "    ax1.plot(val_interval * np.arange(len(val_error_2)), train_error_2, label='train_error')\n",
    "    ax1.plot(val_interval * np.arange(len(val_error_2)), val_error_2, 'r', label='val_error')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('iteration')\n",
    "    ax1.set_ylabel('error')\n",
    "    ax1.set_ylim([0,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "(64, 1, 80, 80)\n",
      "[[   6.15532541   12.15532589   15.15532589   27.15532494   48.15532684]\n",
      " [  55.15532684   54.15532684   55.15532684   55.15532684   57.15532684]\n",
      " [  -3.84467435   -0.84467441   -4.84467459  -26.84467506  -55.84467316]\n",
      " [  37.15532684   35.15532684   36.15532684   41.15532684   37.15532684]\n",
      " [ 127.15532684  127.15532684  127.15532684  128.15531921  126.15532684]]\n",
      "conv1\n",
      "(64, 32, 78, 78)\n",
      "[[ 20.7451973   19.55051613  19.05369568  17.31184959  14.72920036]\n",
      " [  4.96825647   5.32749414   4.42470884   4.84278822   4.70015574]\n",
      " [  1.82525969  -4.52088213 -10.60671806 -11.44818115 -10.18665504]\n",
      " [  3.55405855   4.52627897   6.59822035   8.99958229  10.3826828 ]\n",
      " [ 10.06680679  10.23117447   9.96443176  10.11453247   9.66341019]]\n",
      "relu1\n",
      "(64, 32, 78, 78)\n",
      "[[ 20.7451973   19.55051613  19.05369568  17.31184959  14.72920036]\n",
      " [  4.96825647   5.32749414   4.42470884   4.84278822   4.70015574]\n",
      " [  1.82525969  -0.45208821  -1.06067181  -1.14481819  -1.01866555]\n",
      " [  3.55405855   4.52627897   6.59822035   8.99958229  10.3826828 ]\n",
      " [ 10.06680679  10.23117447   9.96443176  10.11453247   9.66341019]]\n",
      "pool1\n",
      "(64, 32, 39, 39)\n",
      "[[ 20.7451973   19.05369568  15.28434563  16.06189346   9.98182869]\n",
      " [  5.32749414   5.72877264   5.05008554   5.28275061   5.17664194]\n",
      " [  1.82525969  -1.02456629  -0.38554922  12.89453602   5.13438511]\n",
      " [  7.08841085   9.84496498  11.12811947  10.95921803   9.44330502]\n",
      " [ 10.23117447  10.11453247   9.66341019   7.7704463    2.32019758]]\n",
      "conv2\n",
      "(64, 64, 37, 37)\n",
      "[[-7.64142704 -6.51719093 -4.6834321  -2.02294469  2.94304156]\n",
      " [-7.1701293  -6.11477423 -4.93034124 -3.35156655 -3.15914464]\n",
      " [ 4.38443756  9.73897266  8.02218914 -2.99911141 -0.75184441]\n",
      " [-2.51657891 -3.80936408 -5.26592255 -3.75657344  1.17402542]\n",
      " [-1.99453187 -1.47089374  0.80035639  4.84706879  8.36759377]]\n",
      "relu2\n",
      "(64, 64, 37, 37)\n",
      "[[-0.76414269 -0.65171909 -0.46834323 -0.20229447  2.94304156]\n",
      " [-0.71701294 -0.61147743 -0.49303412 -0.33515665 -0.31591448]\n",
      " [ 4.38443756  9.73897266  8.02218914 -0.29991114 -0.07518444]\n",
      " [-0.2516579  -0.38093641 -0.52659225 -0.37565735  1.17402542]\n",
      " [-0.19945319 -0.14708938  0.80035639  4.84706879  8.36759377]]\n",
      "pool2\n",
      "(64, 64, 19, 19)\n",
      "[[ -0.30126831   0.41858312   2.94304156   2.4321785    7.98599482]\n",
      " [  3.61802697   1.8042388   -0.21506193  -0.29506382  -0.30043218]\n",
      " [ 12.37689114   8.02218914   4.49973106  11.08878708   2.85976577]\n",
      " [ -0.2516579    1.62666821   8.2926302    5.72059917   5.27712584]\n",
      " [  0.70333648  10.75233364   9.62697029   3.50987339   8.42336464]]\n",
      "fc1\n",
      "[[-1.17767358 -0.92205566 -0.54156649 -2.03763747 -1.5686202 ]\n",
      " [-1.0201081  -1.50079226 -1.61314678 -2.00165677 -2.62181354]]\n",
      "score\n",
      "[[ 49.37382126  21.4328804   15.67334747  21.32895088  43.77396393\n",
      "   22.34859276  55.38753128  22.33106804  20.90602303  22.55278969\n",
      "    9.26003551  22.24033356  41.86949158  14.72451591  61.59037781\n",
      "   14.65027237  23.63521004  14.3556509    3.64624405  14.09132481\n",
      "   31.26633453  36.93222427  43.49540329  59.52855682  18.15223885\n",
      "   59.20680237  31.29924202  53.31235123  30.79340744  67.72994995]\n",
      " [ 67.15137482  23.82206726  31.50676346  22.92157173  61.77588654\n",
      "   23.79668045  74.22320557  25.12285614  37.07809448  23.05763054\n",
      "   25.10473824  24.39547729  56.80580902  12.97509098  78.75840759\n",
      "   14.04759789  40.72964478  12.51261425  19.09643555  13.40434837\n",
      "   48.61858749  47.34168625  62.02374649  56.509758    36.50154495\n",
      "   55.16471481  48.99770355  53.68177032  48.30317688  63.27226257]]\n",
      "conv1 weights\n",
      "(32, 1, 3, 3)\n",
      "[[[[-0.13301915  0.0157665   0.06198125]\n",
      "   [ 0.07339708 -0.29404196  0.13266848]\n",
      "   [ 0.05566424 -0.10325248  0.27394015]]]\n",
      "\n",
      "\n",
      " [[[-0.45737004 -0.04063667  0.57747513]\n",
      "   [-0.51670009 -0.44888437  0.15098658]\n",
      "   [ 0.50396651  0.42996433 -0.27954903]]]]\n",
      "conv1 biases\n",
      "(32,)\n",
      "[-0.18078275  0.23724432 -0.15263709 -0.18068753  0.31281176]\n",
      "conv2 weights\n",
      "(64, 32, 3, 3)\n",
      "[[[-0.07753047 -0.02039925  0.05603667]\n",
      "  [-0.29143789 -0.07900703  0.02334287]]\n",
      "\n",
      " [[ 0.10382804 -0.02465879 -0.02160467]\n",
      "  [-0.24982168 -0.30859923 -0.10057124]]]\n",
      "conv2 biases\n",
      "(64,)\n",
      "[-0.20324299 -0.18650104 -0.32046983 -0.257231   -0.11465157]\n",
      "fc1 weights\n",
      "(500, 5184)\n",
      "[[ 0.02362981  0.02890437 -0.0751954  -0.01682951  0.00352118]\n",
      " [ 0.03582966  0.01684428 -0.04599937  0.05987068  0.07697855]\n",
      " [-0.02161938  0.03218165 -0.00598135  0.04165873  0.01205852]\n",
      " [ 0.09932558  0.05228741  0.09579293  0.0533229  -0.04455799]\n",
      " [-0.00522006 -0.04690551 -0.09212406 -0.01743576  0.00339798]]\n",
      "fc1 biases\n",
      "(500,)\n",
      "[-0.2515204  -0.17178097 -0.09016043 -0.03916841 -0.02692924]\n",
      "score weights\n",
      "(30, 500)\n",
      "[[ 0.00161732  0.00818002 -0.01715766  0.02327765  0.01825732]\n",
      " [ 0.03304723  0.03893369 -0.00394033  0.00245535 -0.00119911]\n",
      " [ 0.01835795  0.02850597  0.0197205   0.03890863 -0.00253197]\n",
      " [-0.0279687   0.00579735  0.00243413 -0.02139961 -0.01142667]\n",
      " [ 0.01132109  0.0040416  -0.01560521 -0.02662288  0.02607361]]\n",
      "score biases\n",
      "(30,)\n",
      "[ 0.69137609  0.50930411  0.41648787  0.51718628  0.66349447]\n",
      "diffs\n",
      "conv1 weights\n",
      "(32, 1, 3, 3)\n",
      "[[[[ -5.65793089e-05  -2.84530634e-05  -5.43029955e-06]\n",
      "   [ -1.08307620e-04  -7.93494037e-05  -5.14348649e-05]\n",
      "   [ -1.80277464e-04  -1.40315387e-04  -8.63060486e-05]]]\n",
      "\n",
      "\n",
      " [[[ -8.72568125e-05  -7.17651419e-05  -7.91724087e-05]\n",
      "   [ -6.90579618e-05  -4.01393700e-05  -4.56015223e-05]\n",
      "   [ -6.71845701e-05  -4.81395437e-05  -5.29154422e-05]]]]\n",
      "conv1 biases\n",
      "(32,)\n",
      "[  1.98703056e-04  -2.16934859e-05  -7.12575784e-05   1.07189306e-04\n",
      "  -2.17059540e-04]\n",
      "conv2 weights\n",
      "(64, 32, 3, 3)\n",
      "[[[  3.72016402e-05   3.44289147e-05   3.56668570e-05]\n",
      "  [ -7.85602097e-05  -1.40851416e-05   1.55402755e-04]]\n",
      "\n",
      " [[  8.01000569e-05   4.39210489e-05   1.67363596e-05]\n",
      "  [  4.91816972e-05   1.56286202e-04   1.19201235e-04]]]\n",
      "conv2 biases\n",
      "(64,)\n",
      "[  4.93489606e-05   1.93598331e-04   8.78863793e-05  -8.73424942e-05\n",
      "   1.97326575e-04]\n",
      "fc1 weights\n",
      "(500, 5184)\n",
      "[[  1.62271408e-05  -6.81410966e-05   4.90026432e-05  -3.00881275e-05\n",
      "   -5.23452763e-05]\n",
      " [  3.44858243e-04   1.56389535e-04   1.82435178e-05   4.30463588e-05\n",
      "    2.18500063e-05]\n",
      " [  8.08939440e-05  -6.97890709e-06  -3.16395540e-06   1.11428388e-04\n",
      "    1.40426477e-04]\n",
      " [  4.65456578e-05   6.40345170e-05  -2.42764199e-05  -7.32942317e-06\n",
      "    7.03686310e-05]\n",
      " [ -7.08650987e-06   9.11652569e-06   4.16145558e-05  -6.34231328e-05\n",
      "    6.49524954e-05]]\n",
      "fc1 biases\n",
      "(500,)\n",
      "[ -6.97406213e-05   8.43780508e-05   8.70853837e-05   1.34514703e-04\n",
      "   7.67289384e-05]\n",
      "score weights\n",
      "(30, 500)\n",
      "[[  9.71925401e-05   1.38341959e-04   2.92958837e-04   6.55112817e-05\n",
      "    1.55620291e-04]\n",
      " [  2.42393697e-04   2.15225926e-04   1.28804706e-04   6.37896010e-05\n",
      "    1.83232420e-04]\n",
      " [  1.40821707e-04   8.79902946e-05   8.12834624e-05   1.84345889e-04\n",
      "    7.15569913e-05]\n",
      " [  2.28363773e-04   1.56622220e-04   1.61085627e-04   9.76137817e-05\n",
      "    1.46289036e-04]\n",
      " [  1.19382057e-04   1.67967068e-04   3.77275835e-04   1.18641990e-04\n",
      "    2.12856467e-04]]\n",
      "score biases\n",
      "(30,)\n",
      "[-0.00018682 -0.00020737 -0.00015348 -0.00020123 -0.00022303]\n",
      "31.2479141221\n"
     ]
    }
   ],
   "source": [
    "my_net = solver.net\n",
    "train_error_sanity = 0\n",
    "niter_sanity = 1\n",
    "batch_size_used = 128\n",
    "for it in range(niter_sanity):\n",
    "    it_range = range(it*batch_size, it*batch_size+batch_size)\n",
    "    my_net.blobs['data'].data[...] = X_train_clean_cv[it_range]\n",
    "    my_net.forward()\n",
    "    out = my_net.blobs['score'].data\n",
    "    \n",
    "    net = my_net\n",
    "    \n",
    "    print \"data\"\n",
    "    print net.blobs['data'].data.shape\n",
    "    print net.blobs['data'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"conv1\"\n",
    "    print net.blobs['conv1'].data.shape\n",
    "    print net.blobs['conv1'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"relu1\"\n",
    "    print net.blobs['relu1'].data.shape\n",
    "    print net.blobs['relu1'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"pool1\"\n",
    "    print net.blobs['pool1'].data.shape\n",
    "    print net.blobs['pool1'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    \n",
    "    print \"conv2\"\n",
    "    print net.blobs['conv2'].data.shape\n",
    "    print net.blobs['conv2'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"relu2\"\n",
    "    print net.blobs['relu2'].data.shape\n",
    "    print net.blobs['relu2'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"pool2\"\n",
    "    print net.blobs['pool2'].data.shape\n",
    "    print net.blobs['pool2'].data[0:5, 0, 0:5, 0]\n",
    "    \n",
    "    print \"fc1\"\n",
    "    print net.blobs['fc1'].data[0:2,0:5]\n",
    "    \n",
    "    print \"score\"\n",
    "    print net.blobs['score'].data[0:2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print \"conv1 weights\"\n",
    "    print net.params['conv1'][0].data.shape\n",
    "    print net.params['conv1'][0].data[0:2][0:5]\n",
    "    \n",
    "    print \"conv1 biases\"\n",
    "    print net.params['conv1'][1].data.shape\n",
    "    print net.params['conv1'][1].data[0:5]\n",
    "    \n",
    "    print \"conv2 weights\"\n",
    "    print net.params['conv2'][0].data.shape\n",
    "    print net.params['conv2'][0].data[0:2, 0:2, 0:5, 0]\n",
    "    \n",
    "    print \"conv2 biases\"\n",
    "    print net.params['conv2'][1].data.shape\n",
    "    print net.params['conv2'][1].data[0:5]\n",
    "    \n",
    "    print \"fc1 weights\"\n",
    "    print net.params['fc1'][0].data.shape\n",
    "    print net.params['fc1'][0].data[0:5, 0:5]\n",
    "    \n",
    "    print \"fc1 biases\"\n",
    "    print net.params['fc1'][1].data.shape\n",
    "    print net.params['fc1'][1].data[0:5]\n",
    "    \n",
    "    print \"score weights\"\n",
    "    print net.params['score'][0].data.shape\n",
    "    print net.params['score'][0].data[0:5, 0:5]\n",
    "    \n",
    "    print \"score biases\"\n",
    "    print net.params['score'][1].data.shape\n",
    "    print net.params['score'][1].data[0:5]\n",
    "    \n",
    "    \n",
    "    print \"diffs\"\n",
    "    print \"conv1 weights\"\n",
    "    print net.params['conv1'][0].diff.shape\n",
    "    print net.params['conv1'][0].diff[0:2][0:5]\n",
    "    \n",
    "    print \"conv1 biases\"\n",
    "    print net.params['conv1'][1].diff.shape\n",
    "    print net.params['conv1'][1].diff[0:5]\n",
    "    \n",
    "    print \"conv2 weights\"\n",
    "    print net.params['conv2'][0].diff.shape\n",
    "    print net.params['conv2'][0].diff[0:2, 0:2, 0:5, 0]\n",
    "    \n",
    "    print \"conv2 biases\"\n",
    "    print net.params['conv2'][1].diff.shape\n",
    "    print net.params['conv2'][1].diff[0:5]\n",
    "    \n",
    "    print \"fc1 weights\"\n",
    "    print net.params['fc1'][0].diff.shape\n",
    "    print net.params['fc1'][0].diff[0:5, 0:5]\n",
    "    \n",
    "    print \"fc1 biases\"\n",
    "    print net.params['fc1'][1].diff.shape\n",
    "    print net.params['fc1'][1].diff[0:5]\n",
    "    \n",
    "    print \"score weights\"\n",
    "    print net.params['score'][0].diff.shape\n",
    "    print net.params['score'][0].diff[0:5, 0:5]\n",
    "    \n",
    "    print \"score biases\"\n",
    "    print net.params['score'][1].diff.shape\n",
    "    print net.params['score'][1].diff[0:5]\n",
    "    \n",
    "#     print \"loss\"\n",
    "#     print net.params['loss'][0].diff.shape\n",
    "#     print net.params['loss'][0].diff[0:5]\n",
    "    \n",
    "#     train_error += np.sum( (out - y_train_clean_cv[it_range]) ** 2) / float(2* y_train_clean_cv.shape[1])\n",
    "    train_error_sanity += euclidean_loss(out, y_train_clean_cv[it_range])\n",
    "train_error_sanity = train_error_sanity / float(niter_sanity)  \n",
    "\n",
    "print train_error_sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284.23410715\n"
     ]
    }
   ],
   "source": [
    "val_error_sanity = 0\n",
    "niter_sanity = 8\n",
    "batch_size_used = 64\n",
    "for it in range(niter_sanity):\n",
    "    it_range = range(it*batch_size, it*batch_size+batch_size)\n",
    "    my_net.blobs['data'].data[...] = X_val_clean_cv[it_range]\n",
    "    my_net.forward()\n",
    "    out = my_net.blobs['score'].data\n",
    "    \n",
    "#     train_error += np.sum( (out - y_train_clean_cv[it_range]) ** 2) / float(2* y_train_clean_cv.shape[1])\n",
    "    val_error_sanity += euclidean_loss(out, y_val_clean_cv[it_range])\n",
    "val_error_sanity = val_error_sanity / float(niter_sanity)\n",
    "\n",
    "print val_error_sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.0375438094\n"
     ]
    }
   ],
   "source": [
    "val_error_sanity = 0\n",
    "niter_sanity = 8\n",
    "batch_size_used = 64\n",
    "for it in range(niter_sanity):\n",
    "    it_range = range(it*batch_size, it*batch_size+batch_size)\n",
    "    solver.test_nets[0].blobs['data'].data[...] = X_val_clean_cv[it_range]\n",
    "    solver.test_nets[0].forward(start='conv1')\n",
    "    out = solver.test_nets[0].blobs['score'].data\n",
    "    \n",
    "#     train_error += np.sum( (out - y_train_clean_cv[it_range]) ** 2) / float(2* y_train_clean_cv.shape[1])\n",
    "    val_error_sanity += np.sum ( (out - y_val_clean_cv[it_range]) ** 2)\n",
    "\n",
    "val_error_sanity = val_error_sanity / float(niter_sanity * 2 * batch_size_used)\n",
    "\n",
    "print val_error_sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG_MSGS:\n",
    "    print len(conv1_out)\n",
    "    print conv1_out[0].shape\n",
    "\n",
    "    for i in range(5):\n",
    "        conv1_out_i = conv1_out[i]\n",
    "        print conv1_out_i[0:5, 0, 0:5, 0]\n",
    "\n",
    "    print len(conv2_out)\n",
    "    print conv2_out[0].shape\n",
    "\n",
    "    for i in range(5):\n",
    "        conv2_out_i = conv2_out[i]\n",
    "        print conv2_out_i[0:5, 0, 0:5, 0]\n",
    "\n",
    "    print \"conv1 weights and biases\"\n",
    "    print len(conv1_weights)\n",
    "    print conv1_weights[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_weights_i = conv1_weights[i]\n",
    "        print conv1_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print len(conv1_biases)\n",
    "    print conv1_biases[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_biases_i = conv1_biases[i]\n",
    "        print conv1_biases_i[0:5]\n",
    "\n",
    "    print \"diff conv1 weights and biases\"\n",
    "    print len(conv1_weights_diff)\n",
    "    print conv1_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_weights_i = conv1_weights_diff[i]\n",
    "        print conv1_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print len(conv1_biases_diff)\n",
    "    print conv1_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv1_biases_i = conv1_biases_diff[i]\n",
    "        print conv1_biases_i[0:5]\n",
    "\n",
    "    print \"conv2 weights and biases\"\n",
    "    print len(conv2_weights)\n",
    "    print conv2_weights[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_weights_i = conv2_weights[i]\n",
    "        print conv2_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print len(conv2_biases)\n",
    "    print conv2_biases[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_biases_i = conv2_biases[i]\n",
    "        print conv2_biases_i[0:5]\n",
    "\n",
    "    print len(conv2_weights_diff)\n",
    "    print conv2_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_weights_i = conv2_weights_diff[i]\n",
    "        print conv2_weights_i[0:2,0,0:2,0]\n",
    "\n",
    "    print \"diff conv2 weights and biases\"\n",
    "    print len(conv2_biases_diff)\n",
    "    print conv2_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        conv2_biases_i = conv2_biases_diff[i]\n",
    "        print conv2_biases_i[0:5]\n",
    "\n",
    "    print len(fc1_weights_diff)\n",
    "    print fc1_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        fc1_weights_i = fc1_weights_diff[i]\n",
    "        print fc1_weights_i[0:2,0:5]\n",
    "\n",
    "    print len(fc1_biases_diff)\n",
    "    print fc1_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        fc1_biases_i = fc1_biases_diff[i]\n",
    "        print fc1_biases_i[0:5]\n",
    "\n",
    "    print len(score_weights)\n",
    "    print score_weights[0].shape\n",
    "    for i in range(5):\n",
    "        score_weights_i = score_weights[i]\n",
    "        print score_weights_i[0:30:10,0]\n",
    "\n",
    "    print len(score_biases)\n",
    "    print score_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        score_biases_i = score_biases[i]\n",
    "        print score_biases_i[0:30:6]\n",
    "\n",
    "    print len(score_weights_diff)\n",
    "    print score_weights_diff[0].shape\n",
    "    for i in range(5):\n",
    "        score_weights_i = score_weights_diff[i]\n",
    "        print score_weights_i[0:30:10,0]\n",
    "\n",
    "    print len(score_biases_diff)\n",
    "    print score_biases_diff[0].shape\n",
    "    for i in range(5):\n",
    "        score_biases_i = score_biases_diff[i]\n",
    "        print score_biases_i[0:30:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

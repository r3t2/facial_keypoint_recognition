{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "caffe_root = '../../../caffe/'\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "from caffe import layers as L, params as P\n",
    "\n",
    "import h5py\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../data/train_data_cleaned.npz\n",
      "loaded:  ['X_val_clean_cv', 'y_val_clean_cv', 'y_train_clean_cv', 'feature_labels', 'X_train_clean_cv']\n",
      "(1640, 1, 96, 96) (1640, 30)\n",
      "(500, 1, 96, 96) (500, 30)\n"
     ]
    }
   ],
   "source": [
    "np_loaded_data_file = '../data/train_data_cleaned.npz'\n",
    "if not os.path.isfile(np_loaded_data_file):\n",
    "    print \"%s does not exist. See facial_recog_kaggle.ipynb\" % np_loaded_data_file\n",
    "else:\n",
    "    print \"loading %s\" % np_loaded_data_file\n",
    "    npzfile = np.load(np_loaded_data_file)\n",
    "    print \"loaded: \", npzfile.files\n",
    "    X_train_clean_cv, y_train_clean_cv = npzfile['X_train_clean_cv'], npzfile['y_train_clean_cv']\n",
    "    X_val_clean_cv, y_val_clean_cv = npzfile['X_val_clean_cv'], npzfile['y_val_clean_cv']\n",
    "    feature_labels = npzfile['feature_labels']\n",
    "    print X_train_clean_cv.shape, y_train_clean_cv.shape\n",
    "    print X_val_clean_cv.shape, y_val_clean_cv.shape\n",
    "\n",
    "\n",
    "f = h5py.File('../data/train_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_train_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_train_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.close()\n",
    "\n",
    "f = h5py.File('../data/test_data_lenet.hd5', 'w')\n",
    "f.create_dataset('data', data=X_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.create_dataset('label', data=y_val_clean_cv, compression='gzip', compression_opts=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(96, 2.6774287305930625e-05, 1.2685843885889934e-06), (96, 2.6452968002151156e-06, 0.001499378173512326), (64, 0.00047761349214236841, 0.0042614849125783266), (128, 1.6272285424801794e-06, 4.1919335954933465e-05), (96, 2.0233359032726703e-05, 7.7271364406277302e-05), (96, 1.2119310762665393e-05, 0.00056575337039514858), (96, 9.9542320282277079e-06, 0.041471819316919727), (96, 0.00014221530264473372, 0.000484055829283041), (32, 3.5441959061272491e-05, 0.00035792800395328066), (96, 0.00058964999302223966, 0.091607222356769158)]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "td_size = X_train_clean_cv.shape[0]\n",
    "# td_size = 50\n",
    "val_data_size = X_val_clean_cv.shape[0]\n",
    "num_labels = y_train_clean_cv.shape[1]\n",
    "\n",
    "num_cv_points = 10\n",
    "\n",
    "batch_size_vec = np.random.choice([32, 64, 96, 128], num_cv_points, replace=True)\n",
    "\n",
    "base_learning_rate_exps = np.random.uniform(-3, -6, num_cv_points)\n",
    "base_learning_rate = np.power(10, base_learning_rate_exps)\n",
    "\n",
    "reg_param_exps = np.random.uniform(-1, -6, num_cv_points)\n",
    "reg_param_vec = np.power(10, reg_param_exps)\n",
    "\n",
    "print zip(batch_size_vec, base_learning_rate, reg_param_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from net_archs import lenet\n",
    "from def_solver import create_solver\n",
    "\n",
    "train_net_path = 'train_net.prototxt'\n",
    "test_net_path = 'test_net.prototxt'\n",
    "solver_config_fname_base = 'lenet_solver'\n",
    "\n",
    "results = {}\n",
    "for batch_size, base_lr, reg_param in zip(batch_size_vec, base_learning_rate, reg_param_vec):\n",
    "    \n",
    "    train_suffix = \"__b_lr_%.02e__reg_param_%0.02e__batch_size_%d\" % (base_lr, reg_param, batch_size)\n",
    "    print \"train_suffix = %s\" % train_suffix\n",
    "    \n",
    "    with open(train_net_path, 'w') as f:\n",
    "        f.write(str(lenet(hdf5_list='../data/train_hdf5.list', batch_size=batch_size)))\n",
    "\n",
    "    with open(test_net_path, 'w') as f:\n",
    "        f.write(str(lenet(hdf5_list='../data/test_hdf5.list', batch_size=batch_size)))\n",
    "    \n",
    "    s = create_solver(base_lr, reg_param, train_net_path, test_net_path)\n",
    "    solver_config_fname = solver_config_fname_base + train_suffix + \".prototxt\"\n",
    "    with open(solver_config_fname, 'w') as f:\n",
    "        f.write(str(s))\n",
    "    \n",
    "    solver = None\n",
    "    solver = caffe.get_solver(solver_config_fname)\n",
    "    \n",
    "    num_iter_per_epoch = int(np.ceil(float(td_size) / batch_size))\n",
    "    niter = num_iter_per_epoch * num_epochs\n",
    "\n",
    "    niter_val = int(np.ceil( float(val_data_size) / batch_size))\n",
    "    val_interval = num_iter_per_epoch\n",
    "\n",
    "    print \"batch_size = %d, base_learning_rate = %e, reg_param = %e\" % (batch_size, base_lr, reg_param)\n",
    "    \n",
    "    print \"niter = %d, val_interval = %d\" % (niter, val_interval)\n",
    "\n",
    "    train_error = np.zeros(niter)\n",
    "    val_error = np.zeros( int(np.ceil(float(niter) / val_interval)))\n",
    "\n",
    "    for it in range(niter):\n",
    "        solver.step(1)\n",
    "\n",
    "        train_error[it] = solver.net.blobs['loss'].data\n",
    "\n",
    "        if (it % val_interval) == 0:\n",
    "\n",
    "            val_error_this = 0\n",
    "            for test_it in range(niter_val):\n",
    "                solver.test_nets[0].forward()\n",
    "                val_error_this += solver.test_nets[0].blobs['loss'].data / niter_val\n",
    "\n",
    "            val_error[it // val_interval] = val_error_this\n",
    "\n",
    "            print \"it = %d, train_error = %e, val_error = %e\" % (it, train_error[it], val_error[it // val_interval])\n",
    "            \n",
    "        results[(batch_size, base_lr, reg_param)] = (train_error, val_error)\n",
    "        \n",
    "        # save model\n",
    "        model_save_name = \"lenet\" + train_suffix + \".caffemodel\"\n",
    "        solver.net.save(model_save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
